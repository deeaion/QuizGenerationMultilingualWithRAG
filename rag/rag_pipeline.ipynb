{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:25.213830Z",
     "start_time": "2025-04-25T22:04:25.209141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from deepeval.evaluate import EvaluationResult\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n"
   ],
   "id": "de2919a0a35b15b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "class GemmaQuestionTemplates:\n",
    "    def __init__(self):\n",
    "        self.templates = self._initialize_templates()\n",
    "    \n",
    "    def _initialize_templates(self) -> Dict[str, str]:\n",
    "        return {\n",
    "            \"mcq\": self._create_mcq_template(),\n",
    "            \"true_false\": self._create_tf_template(),\n",
    "            \"fill_in_blank\": self._create_fill_in_blank_template(),\n",
    "            \"open_ended\": self._create_open_ended_template(),\n",
    "            \"short_answer\": self._create_short_answer_template()\n",
    "        }\n",
    "    \n",
    "    def _create_mcq_template(self) -> str:\n",
    "        return \"\"\"You are a teacher at a university creating a Multiple Choice Question for your students learning about {course}.\n",
    "\n",
    "The question difficulty should be {difficulty}/5, where:\n",
    "1 - Beginner (no prior knowledge)\n",
    "2 - Basic (some familiarity)\n",
    "3 - Intermediate (general understanding)\n",
    "4 - Advanced (good understanding)\n",
    "5 - Expert (deep understanding)\n",
    "\n",
    "Based on this context:\n",
    "{context}\n",
    "\n",
    "Create exactly ONE multiple-choice question following these steps:\n",
    "1. Read and understand the context\n",
    "2. Extract the key concept based on the difficulty level\n",
    "3. Create a clear question\n",
    "4. Generate one correct answer and three plausible wrong answers\n",
    "5. Write a brief explanation (under 100 words)\n",
    "\n",
    "Format your response exactly like this:\n",
    "Question: [question text]\n",
    "A. [option A]\n",
    "B. [option B]\n",
    "C. [option C]\n",
    "D. [option D]\n",
    "Answer: [correct letter]\n",
    "Explanation: [brief explanation]\n",
    "\n",
    "End your response after the explanation.\n",
    "\"\"\"\n",
    "    \n",
    "    def _create_tf_template(self) -> str:\n",
    "        return \"\"\"You are a teacher creating a True/False question for your students in {course}.\n",
    "\n",
    "The question difficulty should be {difficulty}/5, where:\n",
    "1 - Beginner (no prior knowledge)\n",
    "2 - Basic (some familiarity)\n",
    "3 - Intermediate (general understanding)\n",
    "4 - Advanced (good understanding)\n",
    "5 - Expert (deep understanding)\n",
    "\n",
    "Based on this context:\n",
    "{context}\n",
    "\n",
    "Create exactly ONE True/False question following these steps:\n",
    "1. Read and understand the context\n",
    "2. Identify a key fact that can be presented as true OR create a plausible but false statement\n",
    "3. Create a clear statement to evaluate as true or false\n",
    "4. Provide the correct answer\n",
    "5. Write a brief explanation (under 100 words)\n",
    "\n",
    "Format your response exactly like this:\n",
    "Question: [statement to evaluate]\n",
    "Answer: [True or False]\n",
    "Explanation: [brief explanation]\n",
    "\n",
    "End your response after the explanation.\n",
    "\"\"\"\n",
    "    \n",
    "    def _create_fill_in_blank_template(self) -> str:\n",
    "        return \"\"\"You are a teacher creating a Fill-in-the-blank question for your students in {course}.\n",
    "\n",
    "The question difficulty should be {difficulty}/5, where:\n",
    "1 - Beginner (no prior knowledge)\n",
    "2 - Basic (some familiarity)\n",
    "3 - Intermediate (general understanding)\n",
    "4 - Advanced (good understanding)\n",
    "5 - Expert (deep understanding)\n",
    "\n",
    "Based on this context:\n",
    "{context}\n",
    "\n",
    "Create exactly ONE fill-in-the-blank question following these steps:\n",
    "1. Read and understand the context\n",
    "2. Identify an important sentence with a key term that can be blanked out\n",
    "3. Create the question with _____ for the blank\n",
    "4. Provide the correct answer\n",
    "5. Optionally provide 2-3 plausible wrong answers\n",
    "6. Write a brief explanation (under 100 words)\n",
    "\n",
    "Format your response exactly like this:\n",
    "Question: [sentence with _____ for the blank]\n",
    "Answer: [correct answer]\n",
    "Possible wrong answers: [wrong answer 1], [wrong answer 2], [wrong answer 3] (optional)\n",
    "Explanation: [brief explanation]\n",
    "\n",
    "End your response after the explanation.\n",
    "\"\"\"\n",
    "    \n",
    "    def _create_open_ended_template(self) -> str:\n",
    "        return \"\"\"You are creating an open-ended question about {course}.\n",
    "    \n",
    "    Based on this context:\n",
    "    {context}\n",
    "    \n",
    "    Create ONE open-ended question that:\n",
    "    1. Promotes critical thinking\n",
    "    2. Cannot be answered with a simple yes/no\n",
    "    3. Relates to key concepts in the context\n",
    "    \n",
    "    Format your response exactly like this:\n",
    "    Question: [open-ended question]\n",
    "    Guidelines: [3-5 brief bullet points about what to include]\n",
    "    Sample Answer: [brief outline of key points, maximum 150 words]\n",
    "    \n",
    "    Do not use any markdown formatting like bold or italics.\n",
    "    \"\"\"\n",
    "    def _create_short_answer_template(self) -> str:\n",
    "        return \"\"\"You are creating a short-answer question about {course}.\n",
    "    \n",
    "    Based on this context:\n",
    "    {context}\n",
    "    \n",
    "    Create ONE question that can be answered with a single word or very short phrase (1-3 words).\n",
    "    The question should test specific knowledge from the context.\n",
    "    \n",
    "    Format your response exactly like this:\n",
    "    Question: [question text requiring a short/one-word answer]\n",
    "    Answer: [the correct short answer]\n",
    "    Explanation: [brief explanation why this is correct]\n",
    "    \n",
    "    Example format:\n",
    "    Question: Who invented the World Wide Web?\n",
    "    Answer: Tim Berners-Lee\n",
    "    Explanation: Tim Berners-Lee invented the World Wide Web in 1989 while working at CERN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_template(self, question_type: str) -> str:\n",
    "        if question_type not in self.templates:\n",
    "            raise ValueError(f\"Unknown question type: {question_type}. Available types: {list(self.templates.keys())}\")\n",
    "        return self.templates[question_type]\n",
    "    \n",
    "    def format_template(self, question_type: str, **kwargs) -> str:\n",
    "        template = self.get_template(question_type)\n",
    "        try:\n",
    "            return template.format(**kwargs)\n",
    "        except KeyError as e:\n",
    "            missing_key = str(e).strip(\"'\")\n",
    "            raise ValueError(f\"Missing required parameter: {missing_key} for {question_type} template\")\n",
    "        \n",
    "    def get_all_template_types(self) -> List[str]:\n",
    "        return list(self.templates.keys())\n",
    "    \n",
    "    def add_custom_template(self, template_type: str, template_text: str) -> None:\n",
    "        self.templates[template_type] = template_text\n",
    "        \n",
    "    def modify_template(self, template_type: str, template_text: str) -> None:\n",
    "        if template_type not in self.templates:\n",
    "            raise ValueError(f\"Cannot modify unknown template type: {template_type}\")\n",
    "        self.templates[template_type] = template_text"
   ],
   "id": "cfcdca57200fad5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:25.335076Z",
     "start_time": "2025-04-25T22:04:25.321990Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "class GemmaPromptManager:\n",
    "    def __init__(self):\n",
    "        self.templates = GemmaQuestionTemplates()\n",
    "        self.max_context_length = 1500  # Default max context length\n",
    "    \n",
    "    def set_max_context_length(self, length: int) -> None:\n",
    "        self.max_context_length = length\n",
    "    \n",
    "    def prepare_prompt(self,\n",
    "                     question_type: str,\n",
    "                     context: str,\n",
    "                     course: str,\n",
    "                     difficulty: int = 3,\n",
    "                     distractors: Optional[List[str]] = None) -> str:\n",
    "        # Truncate context if needed\n",
    "        truncated_context = context[:self.max_context_length]\n",
    "        \n",
    "        # Format distractors if provided\n",
    "        distractor_str = \"\"\n",
    "        if distractors and question_type == \"mcq\":\n",
    "            distractor_str = \", \".join(distractors[:10])\n",
    "        \n",
    "        # Prepare parameters for template\n",
    "        params = {\n",
    "            \"context\": truncated_context,\n",
    "            \"course\": course,\n",
    "            \"difficulty\": difficulty\n",
    "        }\n",
    "        \n",
    "        # Add distractors if available\n",
    "        if distractor_str:\n",
    "            params[\"distractors\"] = distractor_str\n",
    "            \n",
    "        # Format the template\n",
    "        return self.templates.format_template(question_type, **params)\n",
    "    \n",
    "    def get_generation_params(self, creative: bool = False) -> Dict:\n",
    "        if creative:\n",
    "            return {\n",
    "                \"max_new_tokens\": 350,\n",
    "                \"temperature\": 0.8,  \n",
    "                \"top_p\": 0.92,\n",
    "                \"repetition_penalty\": 1.2,\n",
    "                \"do_sample\": True\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"max_new_tokens\": 300,\n",
    "                \"temperature\": 0.7, \n",
    "                \"top_p\": 0.9,\n",
    "                \"repetition_penalty\": 1.3,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                \"do_sample\": True\n",
    "            }"
   ],
   "id": "d2b206e655e2f31b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:32.774885Z",
     "start_time": "2025-04-25T22:04:25.396290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from rouge_score import rouge_scorer\n",
    "from CSMetadataExtractor import CSMetadataExtractor\n",
    "import os\n",
    "import torch\n",
    "from chromadb.utils import embedding_functions"
   ],
   "id": "66af30cf3109bd67",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:32.789848Z",
     "start_time": "2025-04-25T22:04:32.785247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_wiki_files_path=\"rag/en\"\n",
    "files = os.listdir(rag_wiki_files_path)\n",
    "metadata_extractor=CSMetadataExtractor()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:32.806133Z",
     "start_time": "2025-04-25T22:04:32.795850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model=\"sentence-transformers/LaBSE\"\n",
    "llm_model_name=\"google/gemma-2-2b-it\"\n",
    "vector_db_path=\"./rag/chroma_db\"\n"
   ],
   "id": "c22db7bc60b30424",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:32.819324Z",
     "start_time": "2025-04-25T22:04:32.813110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize NLTK components\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stopwords_en=set(stopwords.words('english'))\n",
    "stopwords_ro=set(stopwords.words('romanian'))"
   ],
   "id": "8e7511aa8ca196b7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:40.904065Z",
     "start_time": "2025-04-25T22:04:32.827966Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_model=SentenceTransformer(embedding_model,device=\"cuda\")",
   "id": "9e9a2239904cb09a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:43.849470Z",
     "start_time": "2025-04-25T22:04:40.912122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chroma_client=chromadb.PersistentClient(path=vector_db_path)\n",
    "sentence_ef=embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"sentence-transformers/LaBSE\",\n",
    "  \n",
    "    )"
   ],
   "id": "3458d3f7db606d1e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:43.890100Z",
     "start_time": "2025-04-25T22:04:43.857779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a collection in the chroma client\n",
    "# set the embedding function to use the SentenceTransformer model\n",
    "# set the metadata to use cosine distance\n",
    "# i want to first clean the database and then create a new collection\n",
    "# delete the collection if it exists\n",
    "if \"overall_database\" in chroma_client.list_collections():\n",
    "    chroma_client.delete_collection(\"overall_database\")\n",
    "# create a new collection\n",
    "collection=chroma_client.get_or_create_collection(\n",
    "    name=\"overall_database\",\n",
    "    embedding_function=sentence_ef,\n",
    "    metadata={\"hnsw:space\": \"cosine\"})\n"
   ],
   "id": "93f0f8830c6e0a30",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Functions to initiate the LLM and setup the prompts for Quiz Generation",
   "id": "ee40d534b516641e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:52:14.310382Z",
     "start_time": "2025-04-28T19:51:49.660807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm=None\n",
    "def optimize_model_configuration(quality_preference=\"balanced\", force_gpu=True):\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    global tokenizer, llm\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available, will use CPU\")\n",
    "        force_gpu = False\n",
    "        \n",
    "    if quality_preference == \"speed\":\n",
    "        print(\"Optimizing for maximum speed (4-bit quantization)\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_enable_fp32_cpu_offload=not force_gpu\n",
    "        )\n",
    "    elif quality_preference == \"balanced\":\n",
    "        print(\"Using balanced settings (8-bit quantization)\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=not force_gpu,\n",
    "            llm_int8_skip_modules=[\"lm_head\"]\n",
    "        )\n",
    "    elif quality_preference == \"quality\":\n",
    "        print(\"Optimizing for quality (16-bit precision)\")\n",
    "        quantization_config = None\n",
    "    else:\n",
    "        raise ValueError(\"quality_preference must be 'speed', 'balanced', or 'quality'\")\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if force_gpu and torch.cuda.is_available():\n",
    "        device_map = {\"\": 0} \n",
    "        print(\"Forcing model to load on GPU\")\n",
    "    else:\n",
    "        device_map = \"auto\"\n",
    "    \n",
    "    model_kwargs = {\n",
    "        \"device_map\": device_map,\n",
    "        \"torch_dtype\": torch.float16,\n",
    "    }\n",
    "    \n",
    "    if quantization_config:\n",
    "        model_kwargs[\"quantization_config\"] = quantization_config\n",
    "    \n",
    "    try:\n",
    "        from transformers.utils import is_flash_attn_available\n",
    "        if is_flash_attn_available():\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            print(\"Using Flash Attention 2 for faster inference\")\n",
    "    except:\n",
    "        print(\"Flash Attention not available, using standard attention\")\n",
    "    \n",
    "    try:\n",
    "        llm = AutoModelForCausalLM.from_pretrained(\n",
    "            \"google/gemma-2-2b-it\",\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        device_location = next(llm.parameters()).device\n",
    "        print(f\"Model loaded on: {device_location}\")\n",
    "        \n",
    "        if 'cuda' not in str(device_location) and torch.cuda.is_available():\n",
    "            print(\"Warning: Model loaded on CPU despite CUDA being available\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        if force_gpu:\n",
    "            print(\"Falling back to CPU loading\")\n",
    "            return optimize_model_configuration(quality_preference, False)\n",
    "    \n",
    "    return {\n",
    "        \"model_size\": sum(p.numel() for p in llm.parameters()) / 1e6,\n",
    "        \"model_device\": next(llm.parameters()).device,\n",
    "        \"quantization\": quality_preference\n",
    "    }\n",
    "\n",
    "# hope it works\n",
    "result = optimize_model_configuration(\"speed\", force_gpu=True)\n",
    "print(result)"
   ],
   "id": "2b5394ac82d665e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RAGThesisEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "CUDA memory: 8.59 GB\n",
      "Optimizing for maximum speed (4-bit quantization)\n",
      "Loading model...\n",
      "Forcing model to load on GPU\n",
      "Flash Attention not available, using standard attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n",
      "{'model_size': 1602.203904, 'model_device': device(type='cuda', index=0), 'quantization': 'speed'}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:57.733641Z",
     "start_time": "2025-04-25T22:04:57.728553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template_manager=GemmaQuestionTemplates()\n",
    "# get all available template types\n",
    "template_types=template_manager.get_all_template_types()\n",
    "print(\"Available template types:\")\n",
    "print(template_types)\n",
    "tf_template=template_manager.get_template(\"true_false\")\n",
    "fill_in_the_blank_template=template_manager.get_template(\"fill_in_blank\")\n",
    "open_question_template=template_manager.get_template(\"open_ended\")\n",
    "short_answer=template_manager.get_template(\"short_answer\")\n",
    "\n",
    "\n",
    "eval_rouge=rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    use_stemmer=True,\n",
    ")"
   ],
   "id": "acbf0bc17301491b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available template types:\n",
      "['mcq', 'true_false', 'fill_in_blank', 'open_ended', 'short_answer']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_text(text:str,language:str=\"en\"):\n",
    "    if language==\"en\":\n",
    "        stopwords_set=stopwords_en\n",
    "    elif language==\"ro\":\n",
    "        stopwords_set=stopwords_ro\n",
    "    else:\n",
    "        raise ValueError(\"Language not supported\")\n",
    "    \n",
    "    tokens=word_tokenize(text)\n",
    "    tokens=[lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stopwords_set]\n",
    "    return \" \".join(tokens)"
   ],
   "id": "7c0ef2ec39e3dbab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def semantic_chunk(text:str,chunk_size:int=512):\n",
    "    sentences=sent_tokenize(text)\n",
    "    chunks=[]\n",
    "    current_chunk=\"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk)+len(sentence)<chunk_size:\n",
    "            current_chunk+=sentence+\" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk=sentence+\" \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ],
   "id": "24e72d150dca0f4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed_text(text:str,embedding_model:SentenceTransformer):\n",
    "    return embedding_model.encode(text,show_progress_bar=False)"
   ],
   "id": "83afa425af9a4ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_document(document: Dict[str, Any], chunks: List[str], collection: chromadb.Collection, lang: str = \"en\"):\n",
    "    title = document.get(\"title\", \"Untitled\")\n",
    "    section = document.get(\"section\", \"\")\n",
    "    # create basic metadata\n",
    "    metadata_base = {\n",
    "        \"title\": title,\n",
    "        \"section\": section,\n",
    "        \"language\": lang\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if \"text\" in document and metadata_extractor:\n",
    "            additional_metadata = metadata_extractor.extract_metadata(\n",
    "                text=document[\"text\"],\n",
    "                title=title,\n",
    "                lang=lang \n",
    "            )\n",
    "            \n",
    "            if isinstance(additional_metadata, dict):\n",
    "                if \"content_type\" in additional_metadata:\n",
    "                    metadata_base[\"content_type\"] = additional_metadata[\"content_type\"]\n",
    "                if \"difficulty_level\" in additional_metadata:\n",
    "                    if isinstance(additional_metadata[\"difficulty_level\"], str):\n",
    "                        metadata_base[\"difficulty\"] = additional_metadata[\"difficulty_level\"]\n",
    "                    elif isinstance(additional_metadata[\"difficulty_level\"], dict) and \"level\" in additional_metadata[\"difficulty_level\"]:\n",
    "                        metadata_base[\"difficulty\"] = additional_metadata[\"difficulty_level\"][\"level\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {e}\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{title}_{section}_{i}\"\n",
    "        \n",
    "        metadata = metadata_base.copy()\n",
    "        metadata[\"chunk_id\"] = chunk_id\n",
    "        metadata[\"chunk_index\"] = i\n",
    "        metadata[\"total_chunks\"] = len(chunks)\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            documents=[chunk],\n",
    "            metadatas=[metadata],\n",
    "            ids=[chunk_id]\n",
    "        )\n",
    "        \n",
    "        print(f\"Added chunk {i} of document {title} to collection\")\n",
    "        \n",
    " "
   ],
   "id": "35e1932d30d93c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:57.838209Z",
     "start_time": "2025-04-25T22:04:57.833668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_documents_from_directory(directory: str, language: str = \"en\"):\n",
    "    if not os.path.exists(directory):\n",
    "        raise ValueError(f\"Directory {directory} does not exist\")\n",
    "    \n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            try:\n",
    "                with open(os.path.join(directory, file_name), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    documents = json.load(f)\n",
    "                \n",
    "                for document in documents:\n",
    "                    if \"text\" in document and \"section\" in document and \"title\" in document:\n",
    "                        text = document[\"text\"]\n",
    "                        chunks = semantic_chunk(text)\n",
    "                        add_document(document, chunks, collection, lang=language)\n",
    "                    else:\n",
    "                        print(f\"Document in {file_name} does not have the required fields\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {file_name} is not a JSON file\")"
   ],
   "id": "2a3ec0d1af9a3f1c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " def query_vector_store(query: str, k: int = 5, \n",
    "                         filter_metadata: Optional[Dict] = None) -> List[Dict]:\n",
    "        # Query the collection\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=k,\n",
    "            where=filter_metadata\n",
    "        )\n",
    "        \n",
    "        # Format the results\n",
    "        formatted_results = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            formatted_results.append({\n",
    "                \"text\": results[\"documents\"][0][i],\n",
    "                \"metadata\": results[\"metadatas\"][0][i],\n",
    "                \"distance\": results[\"distances\"][0][i] if \"distances\" in results else None,\n",
    "                \"id\": results[\"ids\"][0][i]\n",
    "            })\n",
    "            \n",
    "        return formatted_results        \n",
    " "
   ],
   "id": "868cd3c022da3dbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:57.922066Z",
     "start_time": "2025-04-25T22:04:57.916972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cleanup_memory():\n",
    "    # the llm leaves memory after each generation -> cleanup, we don't need to keep it\n",
    "    import gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()    "
   ],
   "id": "f7b16f3dd1212cd3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_quiz_question(\n",
    "                         question_type: str, \n",
    "                         context: str, \n",
    "                         course: str, \n",
    "                         difficulty: int = 3,\n",
    "                         distractors: List[str] = None) -> str:\n",
    "    import torch\n",
    "    \n",
    "    prompt_manager = GemmaPromptManager()\n",
    "    \n",
    "    prompt_manager.set_max_context_length(1500)\n",
    "    \n",
    "    prompt = prompt_manager.prepare_prompt(\n",
    "        question_type=question_type,\n",
    "        context=context,\n",
    "        course=course,\n",
    "        difficulty=difficulty,\n",
    "        distractors=distractors\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt prepared for {question_type} question (length: {len(prompt)} chars)\")\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(f\"Token count: {input_ids.input_ids.shape[1]} tokens\")\n",
    "    \n",
    "    # Get generation parameters optimized for Gemma\n",
    "    generation_params = prompt_manager.get_generation_params(creative=(question_type == \"open_ended\"))\n",
    "    \n",
    "    # Generate with Gemma\n",
    "    with torch.no_grad():\n",
    "        outputs = llm.generate(\n",
    "            **input_ids,\n",
    "            **generation_params\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][input_ids.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Clean up the output\n",
    "    cleaned_text = clean_generated_text(generated_text)\n",
    "    \n",
    "    # Validate the output\n",
    "    if len(cleaned_text.strip()) < 10:\n",
    "        print(\"Warning: Generated text is very short or empty!\")\n",
    "        if len(generated_text.strip()) > 10:\n",
    "            print(\"Using raw generated text instead.\")\n",
    "            return generated_text.strip()\n",
    "        else:\n",
    "            return f\"Failed to generate a question for this content. The context might be insufficient.\"\n",
    "    \n",
    "    return cleaned_text"
   ],
   "id": "d86ca5382afbe6fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_generated_text(text):\n",
    "    \"\"\"Clean up generated text for consistent MCQ formatting\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # remove any dashes, quotes, etc.\n",
    "    text = re.sub(r'^[\\s\\-\"\\']+', '', text)\n",
    "    text = re.sub(r'[\\s\\-\"\\']+$', '', text)\n",
    "    \n",
    "    # fix common formatting issues\n",
    "    text = text.replace(\"A.\", \"A. \")\n",
    "    text = text.replace(\"B.\", \"B. \")\n",
    "    text = text.replace(\"C.\", \"C. \")\n",
    "    text = text.replace(\"D.\", \"D. \")\n",
    "    \n",
    "    text = re.sub(r'([A-D]\\.)([\\w])', r'\\1 \\2', text)\n",
    "    \n",
    "    # if answer is missing, try to extract it from the options and explanation\n",
    "    if \"Answer:\" not in text and \"Explanation:\" in text:\n",
    "        explanation = text.split(\"Explanation:\")[1].strip()\n",
    "        \n",
    "        # Look for indicators of the correct answer in the explanation\n",
    "        possible_answers = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        for ans in possible_answers:\n",
    "            if f\"option {ans}\" in explanation.lower() or f\"{ans} is correct\" in explanation.lower():\n",
    "                # Insert the answer before the explanation\n",
    "                parts = text.split(\"Explanation:\")\n",
    "                text = parts[0] + f\"Answer: {ans}\\nExplanation:\" + parts[1]\n",
    "                break\n",
    "    \n",
    "    #  add a placeholder\n",
    "    if \"Answer:\" not in text:\n",
    "        if \"Explanation:\" in text:\n",
    "            parts = text.split(\"Explanation:\")\n",
    "            text = parts[0] + \"Answer: [Missing - please determine from explanation]\\nExplanation:\" + parts[1]\n",
    "        else:\n",
    "            text = text + \"\\nAnswer: [Missing - please determine from context]\\nExplanation: Answer cannot be determined from the generated content.\"\n",
    "    \n",
    "    return text"
   ],
   "id": "f05dabd5eaf6c858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_quiz(\n",
    "                   topic: str, \n",
    "                   course: str, \n",
    "                   num_questions: int = 5,\n",
    "                   question_types: List[str] = None,\n",
    "                   difficulty: int = 3,\n",
    "                   filter_metadata: Optional[Dict] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate a complete quiz using Gemma with optimized prompts.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Initialize default question types if none provided\n",
    "    if question_types is None:\n",
    "        prompt_manager = GemmaPromptManager()\n",
    "        question_types = prompt_manager.templates.get_all_template_types()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Query for relevant chunks\n",
    "    print(f\"Querying for chunks related to: {topic}\")\n",
    "    relevant_chunks = query_vector_store(\n",
    "        query=topic,\n",
    "        k=num_questions * 2,  \n",
    "        filter_metadata=filter_metadata\n",
    "    )\n",
    "    \n",
    "    valid_chunks = [chunk for chunk in relevant_chunks \n",
    "                   if len(chunk[\"text\"].strip()) >= 200]\n",
    "    \n",
    "    if not valid_chunks:\n",
    "        print(\"No chunks with sufficient content were found.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(valid_chunks)} usable chunks out of {len(relevant_chunks)} total chunks\")\n",
    "    \n",
    "    cleanup_memory()\n",
    "    \n",
    "    quiz_questions = []\n",
    "    for i in range(min(num_questions, len(valid_chunks))):\n",
    "        try:\n",
    "            q_type = question_types[i % len(question_types)]\n",
    "            \n",
    "            context = valid_chunks[i][\"text\"]\n",
    "            \n",
    "            print(f\"Generating {q_type} question {i+1}/{min(num_questions, len(valid_chunks))}\")\n",
    "            \n",
    "            distractors = None\n",
    "            if q_type == \"mcq\":\n",
    "                distractors = extract_distractors(valid_chunks, i)\n",
    "            \n",
    "            start = time.time()\n",
    "            generated_question = generate_quiz_question(\n",
    "                question_type=q_type,\n",
    "                context=context,\n",
    "                course=course,\n",
    "                difficulty=difficulty,\n",
    "                distractors=distractors\n",
    "            )\n",
    "            \n",
    "            if len(generated_question.strip()) > 10:\n",
    "                # Add to quiz\n",
    "                quiz_questions.append({\n",
    "                    \"question_type\": q_type,\n",
    "                    \"content\": generated_question,\n",
    "                    \"source_chunk\": {\n",
    "                        \"text\": context[:200] + \"...\" if len(context) > 200 else context,\n",
    "                        \"metadata\": valid_chunks[i][\"metadata\"]\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                end = time.time()\n",
    "                print(f\"Question {i+1} generated in {end-start:.2f} seconds\")\n",
    "            else:\n",
    "                print(f\" Failed to generate valid content for question {i+1}\")\n",
    "                try_backup_chunk(\n",
    "                    quiz_questions, valid_chunks, i, num_questions, \n",
    "                    q_type, course, difficulty, distractors\n",
    "                )\n",
    "            \n",
    "            # Clean up memory between questions\n",
    "            if i < min(num_questions, len(valid_chunks)) - 1:\n",
    "                cleanup_memory()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating question {i+1}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nQuiz generation completed:\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Successfully generated {len(quiz_questions)}/{num_questions} questions\")\n",
    "    \n",
    "    return quiz_questions"
   ],
   "id": "d78c0f6a0dcae00d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_distractors(chunks, current_index, max_distractors=10):\n",
    "    \"\"\"Extract potential distractors from other chunks\"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    distractors = []\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        if j != current_index:\n",
    "            words = word_tokenize(chunk[\"text\"])\n",
    "            potential_distractors = [w for w in words if len(w) > 5 and w.isalpha()]\n",
    "            distractors.extend(potential_distractors[:5]) \n",
    "    \n",
    "    # Remove duplicates and limit\n",
    "    return list(set(distractors))[:max_distractors]"
   ],
   "id": "536c153a3358865e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def try_backup_chunk(quiz_questions, valid_chunks, current_index, num_questions, \n",
    "                    question_type, course, difficulty, distractors):\n",
    "    \"\"\"Try generating with a backup chunk if the primary fails\"\"\"\n",
    "    if current_index + num_questions < len(valid_chunks):\n",
    "        backup_index = current_index + num_questions\n",
    "        backup_chunk = valid_chunks[backup_index]\n",
    "        print(f\"Trying again with a different chunk...\")\n",
    "        \n",
    "        generated_question = generate_quiz_question(\n",
    "            question_type=question_type,\n",
    "            context=backup_chunk[\"text\"],\n",
    "            course=course,\n",
    "            difficulty=difficulty,\n",
    "            distractors=distractors\n",
    "        )\n",
    "        \n",
    "        if len(generated_question.strip()) > 10:\n",
    "            quiz_questions.append({\n",
    "                \"question_type\": question_type,\n",
    "                \"content\": generated_question,\n",
    "                \"source_chunk\": {\n",
    "                    \"text\": backup_chunk[\"text\"][:200] + \"...\" if len(backup_chunk[\"text\"]) > 200 else backup_chunk[\"text\"],\n",
    "                    \"metadata\": backup_chunk[\"metadata\"]\n",
    "                }\n",
    "            })\n",
    "            print(f\"Question generated with backup chunk\")\n",
    "            return True\n",
    "    \n",
    "    print(f\"Failed to generate question with backup chunk\")\n",
    "    return False\n"
   ],
   "id": "9184066466f0a2ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:58.026887Z",
     "start_time": "2025-04-25T22:04:58.022446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load_documents_from_directory(rag_wiki_files_path,language=\"en\")\n",
    "# load_documents_from_directory(\"rag/ro\",language=\"ro\")\n",
    "# # quiz = generate_quiz(\n",
    "#         topic=\"Artificial Intelligence\",\n",
    "#         course=\"Artificial Intelligence\",\n",
    "#         num_questions=10,\n",
    "#         question_types=[\"mcq\", \"true_false\", \"fill_in_blank\", \"open_ended\",\"short_answer\"],\n",
    "#         difficulty=3\n",
    "#     )\n",
    "# print(\"Generated Quiz:\")\n",
    "# print(quiz)\n",
    "# for i, question in enumerate(quiz, 1):\n",
    "#     print(f\"\\nQuestion {i} ({question['question_type']}):\")\n",
    "#     print(question['content'])\n",
    "#     print(\"-\" * 50)\n"
   ],
   "id": "cb3e6f98df7cfd9",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:58.069804Z",
     "start_time": "2025-04-25T22:04:58.065011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # load_documents_from_directory(rag_wiki_files_path,language=\"en\")\n",
    "# # load_documents_from_directory(\"rag/ro\",language=\"ro\")\n",
    "# quiz = generate_quiz(\n",
    "#         topic=\"Artificial Intelligence\",\n",
    "#         course=\"Artificial Intelligence\",\n",
    "#         num_questions=1,\n",
    "#         question_types=[\"short_answer\"],\n",
    "#         difficulty=5\n",
    "#     )\n",
    "# print(\"Generated Quiz:\")\n",
    "# print(quiz)\n",
    "# for i, question in enumerate(quiz, 1):\n",
    "#     print(f\"\\nQuestion {i} ({question['question_type']}):\")\n",
    "#     print(question['content'])\n",
    "#     print(\"-\" * 50)\n",
    " "
   ],
   "id": "79ab9200f04b8ea9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:04:58.112481Z",
     "start_time": "2025-04-25T22:04:58.105383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_rouge(generated_questions, reference_questions):\n",
    "    from rouge_score import rouge_scorer\n",
    "    import re\n",
    "\n",
    "    def extract_core_question(text):\n",
    "        if isinstance(text, dict) and \"content\" in text:\n",
    "            text = text[\"content\"]\n",
    "            \n",
    "        text = re.sub(r'Question:|Answer:|Explanation:|A\\.|B\\.|C\\.|D\\.', '', text)\n",
    "        \n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower()\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    results = {\n",
    "        'rouge1': [],\n",
    "        'rouge2': [],\n",
    "        'rougeL': []\n",
    "    }\n",
    "\n",
    "    for gen_q, ref_q in zip(generated_questions, reference_questions):\n",
    "        gen_text = extract_core_question(gen_q)\n",
    "        ref_text = extract_core_question(ref_q)\n",
    "\n",
    "        if not gen_text or not ref_text:\n",
    "            continue\n",
    "\n",
    "        scores = scorer.score(ref_text, gen_text)\n",
    "        for metric in results.keys():\n",
    "            results[metric].append(scores[metric].fmeasure)\n",
    "\n",
    "    # Average results\n",
    "    avg_results = {\n",
    "        metric: sum(scores) / len(scores) if scores else 0\n",
    "        for metric, scores in results.items()\n",
    "    }\n",
    "\n",
    "    return avg_results"
   ],
   "id": "e4c7d07ec82d7d1e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T20:08:51.609637Z",
     "start_time": "2025-04-28T20:08:51.601035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_deepeval(generated_questions, context_chunks):\n",
    "    try:\n",
    "        from deepeval import evaluate\n",
    "        from deepeval.test_case import LLMTestCase\n",
    "        from deepeval.metrics import AnswerRelevancyMetric\n",
    "        from deepeval.metrics import ContextualRelevancyMetric\n",
    "\n",
    "        available_metrics = []\n",
    "        \n",
    "        available_metrics.append((\"answer_relevancy\", AnswerRelevancyMetric(threshold=0.7)))\n",
    "        print(\"Using AnswerRelevancyMetric\")\n",
    "    \n",
    "        available_metrics.append((\"contextual_relevancy\", ContextualRelevancyMetric(threshold=0.7)))\n",
    "        print(\"Using ContextualRelevancyMetric\")\n",
    "    except ImportError:\n",
    "        print(\"Could not import something. Check metrics \")\n",
    "        return {\"error\": \"DeepEval not installed or not available\"}\n",
    "\n",
    "    # Store results for each metric\n",
    "    results = {metric_name: [] for metric_name, _ in available_metrics}\n",
    "    evaluations = []\n",
    "    \n",
    "    # Extract metrics for evaluation\n",
    "    metric_objects = [metric for _, metric in available_metrics]\n",
    "    \n",
    "    for i, question in enumerate(generated_questions):\n",
    "        # Get question text\n",
    "        if isinstance(question, dict) and \"content\" in question:\n",
    "            q_text = question[\"content\"]\n",
    "        else:\n",
    "            q_text = str(question)\n",
    "\n",
    "        # Try to extract just the question part\n",
    "        if \"Question:\" in q_text:\n",
    "            q_part = q_text.split(\"Question:\")[1]\n",
    "            # Further split if there's an Answer section\n",
    "            if \"Answer:\" in q_part:\n",
    "                q_part = q_part.split(\"Answer:\")[0]\n",
    "            q_part = q_part.strip()\n",
    "        else:\n",
    "            q_part = q_text\n",
    "\n",
    "        # Get context text\n",
    "        context = context_chunks[i].get(\"text\", \"\") if i < len(context_chunks) else \"\"\n",
    "\n",
    "        try:\n",
    "            # Create a single test case\n",
    "            test_case = LLMTestCase(\n",
    "                input=context,\n",
    "                actual_output=q_part\n",
    "            )\n",
    "            \n",
    "            # Run evaluation with available metrics\n",
    "            evaluation_result = evaluate(\n",
    "                test_cases=[test_case],\n",
    "                metrics=metric_objects\n",
    "            )\n",
    "\n",
    "            # Extract scores from results\n",
    "            if evaluation_result and evaluation_result.test_results:\n",
    "                test_result = evaluation_result.test_results[0]\n",
    "                \n",
    "                # Parse each metric result\n",
    "                for j, (metric_name, _) in enumerate(available_metrics):\n",
    "                    if j < len(test_result.metrics_data):\n",
    "                        metric_data = test_result.metrics_data[j]\n",
    "                        results[metric_name].append(metric_data.score)\n",
    "                        \n",
    "                        # Store the full evaluation\n",
    "                        evaluations.append({\n",
    "                            \"metric\": metric_name,\n",
    "                            \"score\": metric_data.score,\n",
    "                            \"reason\": getattr(metric_data, 'reason', None),\n",
    "                            \"question_index\": i\n",
    "                        })\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"DeepEval failed on question {i+1}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    # If no successful evaluations, return error\n",
    "    if not evaluations:\n",
    "        return {\"error\": \"no evaluations completed\"}\n",
    "\n",
    "    # Compute aggregate scores for each metric\n",
    "    aggregated = {}\n",
    "    for metric_name, scores in results.items():\n",
    "        if scores:\n",
    "            aggregated[metric_name] = sum(scores) / len(scores)\n",
    "    \n",
    "    # Add metadata\n",
    "    aggregated[\"num_evaluated\"] = len(evaluations) // len(available_metrics) if available_metrics else 0\n",
    "    aggregated[\"evaluations\"] = evaluations\n",
    "    \n",
    "    return aggregated"
   ],
   "id": "6c9b0ba66c2686c3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !deepeval set-local-model --model-name=\"meta-llama-3.1-8b-instruct\" --base-url=\"http://localhost:4500/v1/\" --api-key=\"test\"",
   "id": "a344f4c4ce5bb26e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from deepeval.key_handler import KEY_FILE_HANDLER, KeyValues\n",
    "\n",
    "print(KEY_FILE_HANDLER.fetch_data(KeyValues.LOCAL_MODEL_NAME))\n",
    "print(KEY_FILE_HANDLER.fetch_data(KeyValues.LOCAL_MODEL_BASE_URL))\n",
    "print(KEY_FILE_HANDLER.fetch_data(KeyValues.LOCAL_MODEL_API_KEY))\n",
    "import os\n",
    "os.environ[\"DEEPEVAL_USE_LOCAL_MODEL\"] = \"YES\"\n"
   ],
   "id": "373fe44bc1b5f567"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:06:21.828176Z",
     "start_time": "2025-04-25T22:06:21.818655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_with_lm_studio(generated_questions, api_url=\"http://localhost:4500/v1/completions\"):\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    evaluation_prompt = \"\"\"\n",
    "    You are an expert evaluator for educational quiz questions. Rate the following question on a scale of 1-10 for:\n",
    "    1. Clarity: How clear and unambiguous is the question?\n",
    "    2. Relevance: How relevant is the question to the subject area?\n",
    "    3. Educational Value: How valuable is this question for learning?\n",
    "    4. Technical Accuracy: How factually correct is the content?\n",
    "    \n",
    "    Question to evaluate:\n",
    "    {question}\n",
    "    \n",
    "    Provide your ratings and a brief explanation for each criterion.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for question in generated_questions:\n",
    "        q_content = question['content'] if isinstance(question, dict) and 'content' in question else str(question)\n",
    "        \n",
    "        request_data = {\n",
    "            \"model\": \"meta-llama-3.1-8b-instruct\",  \n",
    "            \"prompt\": evaluation_prompt.format(question=q_content),\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": 0.3 \n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                api_url,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                data=json.dumps(request_data)\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                evaluation = response.json()[\"choices\"][0][\"text\"]\n",
    "                \n",
    "                import re\n",
    "                ratings = {}\n",
    "                for criterion in [\"Clarity\", \"Relevance\", \"Educational Value\", \"Technical Accuracy\"]:\n",
    "                    match = re.search(f\"{criterion}:\\\\s*(\\\\d+)\", evaluation, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        ratings[criterion.lower().replace(\" \", \"_\")] = int(match.group(1))\n",
    "                \n",
    "                results.append({\n",
    "                    \"question\": q_content[:100] + \"...\", \n",
    "                    \"full_evaluation\": evaluation,\n",
    "                    \"ratings\": ratings\n",
    "                })\n",
    "            else:\n",
    "                print(f\"API request failed with status code: {response.status_code}\")\n",
    "                results.append({\n",
    "                    \"question\": q_content[:100] + \"...\",\n",
    "                    \"error\": f\"API request failed: {response.status_code}\"\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error during LM Studio evaluation: {e}\")\n",
    "            results.append({\n",
    "                \"question\": q_content[:100] + \"...\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n"
   ],
   "id": "49130ece6343db84",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:06:21.901067Z",
     "start_time": "2025-04-25T22:06:21.892844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "help_model=\"meta-llama-3.1-8b-instruct\"\n",
    "def generate_generic_quiz(\n",
    "    topic: str,\n",
    "    course: str,\n",
    "    num_questions: int = 5,\n",
    "    question_types: List[str] = None,\n",
    "    difficulty: int = 3\n",
    ") -> List[Dict]:\n",
    "    # call for lm studio \n",
    "    api_url=\"http://localhost:4500/v1/completions\"\n",
    "    evaluation_prompt = \"\"\" Generate a quiz for the following topic and course.\n",
    "    Topic: {topic}\n",
    "    Course: {course}\n",
    "    Number of questions: {num_questions}\n",
    "    Question types: {question_types}\n",
    "    Difficulty: {difficulty}\n",
    "    \"\"\"\n",
    "    request_data = {\n",
    "        \"model\": help_model,\n",
    "        \"prompt\": evaluation_prompt.format(\n",
    "            topic=topic,\n",
    "            course=course,\n",
    "            num_questions=num_questions,\n",
    "            question_types=\", \".join(question_types) if question_types else \"mcq, true_false\",\n",
    "            difficulty=difficulty\n",
    "        ),\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            data=json.dumps(request_data)\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            generated_questions = response.json()[\"choices\"][0][\"text\"]\n",
    "            print(\"Generated Questions:\")   \n",
    "            print(generated_questions)\n",
    "            questions_list = generated_questions.split(\"\\n\")\n",
    "            questions = []\n",
    "            for q in questions_list:\n",
    "                if q.strip():\n",
    "                    question_dict = {\n",
    "                        \"question_type\": \"generic\",\n",
    "                        \"content\": q.strip(),\n",
    "                        \"source_chunk\": {\n",
    "                            \"text\": topic,\n",
    "                            \"metadata\": {}\n",
    "                        }\n",
    "                    }\n",
    "                    questions.append(question_dict)\n",
    "            return questions\n",
    "        \n",
    "        else:\n",
    "            print(f\"API request failed with status code: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API request: {e}\")\n",
    "        return []\n",
    "    \n"
   ],
   "id": "5f5420830a86d94d",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:06:21.967772Z",
     "start_time": "2025-04-25T22:06:21.958545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_quiz_quality(generated_questions, reference_questions=None):\n",
    "    \n",
    "    results = {\n",
    "        \"format_compliance\": 0,\n",
    "        \"content_relevance\": 0,\n",
    "        \"question_clarity\": 0,\n",
    "        \"overall_quality\": 0\n",
    "    }\n",
    "    \n",
    "    # Format compliance check\n",
    "    format_scores = []\n",
    "    for q in generated_questions:\n",
    "        q_type = q[\"question_type\"]\n",
    "        content = q[\"content\"]\n",
    "        \n",
    "        # Check required elements based on question type\n",
    "        if q_type == \"mcq\":\n",
    "            required = [\"Question:\", \"A.\", \"B.\", \"C.\", \"D.\", \"Answer:\", \"Explanation:\"]\n",
    "        elif q_type == \"true_false\":\n",
    "            required = [\"Question:\", \"Answer:\", \"Explanation:\"]\n",
    "        elif q_type == \"fill_in_blank\":\n",
    "            required = [\"Question:\", \"Answer:\", \"Explanation:\"]\n",
    "        elif q_type == \"short_answer\":\n",
    "            required = [\"Question:\", \"Answer:\", \"Explanation:\"]\n",
    "        elif q_type == \"open_ended\":\n",
    "            required = [\"Question:\", \"Guidelines:\", \"Sample Answer:\"]\n",
    "        else:\n",
    "            required = [\"Question:\"]\n",
    "        \n",
    "        # Calculate format compliance\n",
    "        present = sum(1 for r in required if r in content)\n",
    "        format_score = present / len(required)\n",
    "        format_scores.append(format_score)\n",
    "    \n",
    "    results[\"format_compliance\"] = sum(format_scores) / len(format_scores) if format_scores else 0\n",
    "    \n",
    "    \n",
    "    return results"
   ],
   "id": "700b24b1a302e4dc",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:06:22.023033Z",
     "start_time": "2025-04-25T22:06:22.016109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def benchmark_performance(topic, course):\n",
    "    \"\"\"Benchmark the performance of the RAG system\"\"\"\n",
    "    import time\n",
    "    \n",
    "    metrics = {\n",
    "        \"retrieval_time\": 0,\n",
    "        \"generation_time\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"tokens_per_second\": 0,\n",
    "        \"questions_per_minute\": 0\n",
    "    }\n",
    "    \n",
    "    # Measure overall time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Measure retrieval time\n",
    "    retrieval_start = time.time()\n",
    "    chunks = query_vector_store(\n",
    "        query=topic,\n",
    "        k=5\n",
    "    )\n",
    "    retrieval_time = time.time() - retrieval_start\n",
    "    metrics[\"retrieval_time\"] = retrieval_time\n",
    "    \n",
    "    if chunks:\n",
    "        generation_start = time.time()\n",
    "        question = generate_quiz_question(\n",
    "            question_type=\"mcq\",\n",
    "            context=chunks[0][\"text\"],\n",
    "            course=course,\n",
    "            difficulty=3\n",
    "        )\n",
    "        generation_time = time.time() - generation_start\n",
    "        metrics[\"generation_time\"] = generation_time\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    metrics[\"total_time\"] = time.time() - start_time\n",
    "    metrics[\"questions_per_minute\"] = 60 / metrics[\"generation_time\"] if metrics[\"generation_time\"] > 0 else 0\n",
    "    \n",
    "    return metrics"
   ],
   "id": "584364c50e514f4d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from LSTMTextSimilarity import predict_similarity\n",
    "\n",
    "\n",
    "def evaluate_with_lstm_similarity(generated_questions, reference_questions=None, context_chunks=None, lstm_model=None):\n",
    "    results = {\n",
    "        \"question_to_reference_similarity\": [],\n",
    "        \"question_to_context_similarity\": [],\n",
    "        \"question_to_question_similarity_matrix\": []\n",
    "    }\n",
    "\n",
    "    # Extract just question text\n",
    "    def extract_question(text):\n",
    "        if isinstance(text, dict) and \"content\" in text:\n",
    "            text = text[\"content\"]\n",
    "        if \"Question:\" in text:\n",
    "            return text.split(\"Question:\")[1].split(\"Answer:\")[0].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    # Prepare questions\n",
    "    gen_questions = [extract_question(q) for q in generated_questions]\n",
    "\n",
    "    # Calculate similarity to reference questions (if provided)\n",
    "    if reference_questions:\n",
    "        ref_questions = [extract_question(q) for q in reference_questions]\n",
    "\n",
    "        for i, gen_q in enumerate(gen_questions):\n",
    "            if i < len(ref_questions):\n",
    "                sim_score = predict_similarity(gen_q, ref_questions[i])\n",
    "                results[\"question_to_reference_similarity\"].append(sim_score)\n",
    "\n",
    "    # Calculate similarity to context chunks (if provided)\n",
    "    if context_chunks:\n",
    "        context_texts = [c.get(\"text\", \"\") if isinstance(c, dict) else str(c) for c in context_chunks]\n",
    "\n",
    "        for i, gen_q in enumerate(gen_questions):\n",
    "            if i < len(context_texts):\n",
    "                sim_score = predict_similarity(gen_q, context_texts[i])\n",
    "                results[\"question_to_context_similarity\"].append(sim_score)\n",
    "\n",
    "    # Calculate pairwise similarity between generated questions (for diversity measurement)\n",
    "    similarity_matrix = []\n",
    "    for i, q1 in enumerate(gen_questions):\n",
    "        row = []\n",
    "        for j, q2 in enumerate(gen_questions):\n",
    "            if i == j:  # Same question\n",
    "                row.append(100.0)\n",
    "            else:\n",
    "                sim_score = predict_similarity(q1, q2)\n",
    "                row.append(sim_score)\n",
    "        similarity_matrix.append(row)\n",
    "\n",
    "    results[\"question_to_question_similarity_matrix\"] = similarity_matrix\n",
    "\n",
    "    # Calculate average diversity (100 - average non-diagonal similarity)\n",
    "    if len(similarity_matrix) > 1:\n",
    "        non_diagonal_similarities = []\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(len(similarity_matrix[i])):\n",
    "                if i != j:\n",
    "                    non_diagonal_similarities.append(similarity_matrix[i][j])\n",
    "\n",
    "        avg_similarity = sum(non_diagonal_similarities) / len(non_diagonal_similarities) if non_diagonal_similarities else 0\n",
    "        results[\"diversity_score\"] = 100 - avg_similarity  # Higher value means more diverse questions\n",
    "\n",
    "    # Calculate averages for other metrics\n",
    "    for key in [\"question_to_reference_similarity\", \"question_to_context_similarity\"]:\n",
    "        if results[key]:\n",
    "            results[f\"avg_{key}\"] = sum(results[key]) / len(results[key])\n",
    "\n",
    "    return results"
   ],
   "id": "3466c85680624fe8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Integration with the comprehensive evaluation framework\n",
    "def run_lstm_enhanced_evaluation(topic, course, num_questions=5, lstm_model=None):\n",
    "    # Step 1: Generate questions with RAG system\n",
    "    rag_questions = generate_quiz(\n",
    "        topic=topic,\n",
    "        course=course,\n",
    "        num_questions=num_questions,\n",
    "        question_types=[\"mcq\", \"true_false\", \"short_answer\"]\n",
    "    )\n",
    "\n",
    "    # Step 2: Generate reference questions\n",
    "    reference_questions = generate_generic_quiz(\n",
    "        topic=topic,\n",
    "        course=course,\n",
    "        num_questions=num_questions,\n",
    "        question_types=[\"mcq\", \"true_false\", \"short_answer\"]\n",
    "    )\n",
    "\n",
    "    # Step 3: Get the source contexts used\n",
    "    source_contexts = [q.get(\"source_chunk\", {\"text\": \"\"}) for q in rag_questions]\n",
    "    # Step 5: Run evaluation metrics\n",
    "    results = {\n",
    "        \"basic_metrics\": evaluate_quiz_quality(rag_questions),\n",
    "        \"rouge_metrics\": evaluate_with_rouge(rag_questions, reference_questions),\n",
    "        \"lstm_similarity\": evaluate_with_lstm_similarity(\n",
    "            rag_questions,\n",
    "            reference_questions,\n",
    "            source_contexts,\n",
    "            lstm_model\n",
    "        ),\n",
    "        \"performance_metrics\": benchmark_performance(topic, course)\n",
    "    }\n",
    "\n",
    "    # Step 6: Try to run DeepEval if available\n",
    "    try:\n",
    "        results[\"deepeval_metrics\"] = evaluate_with_deepeval(rag_questions, source_contexts)\n",
    "    except Exception as e:\n",
    "        print(f\"DeepEval evaluation failed: {e}\")\n",
    "        results[\"deepeval_metrics\"] = {\"error\": str(e)}\n",
    "\n",
    "    # Step 7: Display results\n",
    "    print(f\"\\n=== LSTM-Enhanced Evaluation Results for {topic} ===\")\n",
    "\n",
    "    # Display basic metrics\n",
    "    print(f\"Format Compliance: {results['basic_metrics']['format_compliance']:.2f}\")\n",
    "\n",
    "    # Display ROUGE metrics\n",
    "    print(f\"ROUGE-L F1 Score: {results['rouge_metrics']['rougeL']:.4f}\")\n",
    "\n",
    "    # Display LSTM similarity metrics\n",
    "    lstm_sim = results.get(\"lstm_similarity\", {})\n",
    "    if \"avg_question_to_context_similarity\" in lstm_sim:\n",
    "        print(f\"Avg. Context Similarity: {lstm_sim['avg_question_to_context_similarity']:.2f}%\")\n",
    "    if \"avg_question_to_reference_similarity\" in lstm_sim:\n",
    "        print(f\"Avg. Reference Similarity: {lstm_sim['avg_question_to_reference_similarity']:.2f}%\")\n",
    "    if \"diversity_score\" in lstm_sim:\n",
    "        print(f\"Question Diversity Score: {lstm_sim['diversity_score']:.2f}%\")\n",
    "\n",
    "    # Performance metrics\n",
    "    print(f\"Generation Speed: {results['performance_metrics']['questions_per_minute']:.2f} questions/minute\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# Load your LSTM model once\n",
    "# lstm_model = LSTMTextSimilarity(path=\"models/lstm_similarity_model.h5\")\n",
    "# results = run_lstm_enhanced_evaluation(\"Artificial Intelligence\", \"Computer Science\", lstm_model=lstm_model)"
   ],
   "id": "a05d981fa20a7a62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:06:22.144223Z",
     "start_time": "2025-04-25T22:06:22.132403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_question_complexity(generated_questions):\n",
    "    \"\"\"\n",
    "    Analyze complexity metrics of generated questions\n",
    "    \n",
    "    Args:\n",
    "        generated_questions: List of generated questions\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of complexity metrics\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    metrics = {\n",
    "        \"avg_word_count\": 0,\n",
    "        \"avg_sentence_count\": 0,\n",
    "        \"avg_word_length\": 0,\n",
    "        \"readability_scores\": {\n",
    "            \"flesch_reading_ease\": 0,\n",
    "            \"flesch_kincaid_grade\": 0\n",
    "        },\n",
    "        \"question_types\": {\n",
    "            \"factual\": 0,\n",
    "            \"conceptual\": 0,\n",
    "            \"analytical\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    total_words = 0\n",
    "    total_sentences = 0\n",
    "    total_chars = 0\n",
    "    processed = 0\n",
    "    \n",
    "    # Keywords suggesting different question types\n",
    "    factual_keywords = [\"what\", \"when\", \"where\", \"who\", \"list\", \"name\", \"identify\"]\n",
    "    conceptual_keywords = [\"why\", \"how\", \"explain\", \"describe\", \"compare\", \"contrast\", \"define\"]\n",
    "    analytical_keywords = [\"analyze\", \"evaluate\", \"examine\", \"interpret\", \"assess\", \"critique\", \"justify\"]\n",
    "    \n",
    "    for question in generated_questions:\n",
    "        # Extract question text\n",
    "        if isinstance(question, dict) and \"content\" in question:\n",
    "            q_text = question[\"content\"]\n",
    "        else:\n",
    "            q_text = str(question)\n",
    "            \n",
    "        # Extract just the question part if possible\n",
    "        if \"Question:\" in q_text:\n",
    "            q_text = q_text.split(\"Question:\")[1].split(\"Answer:\")[0].strip()\n",
    "        \n",
    "        # Tokenize\n",
    "        words = word_tokenize(q_text.lower())\n",
    "        sentences = sent_tokenize(q_text)\n",
    "        \n",
    "        # Update counters\n",
    "        total_words += len(words)\n",
    "        total_sentences += len(sentences)\n",
    "        total_chars += sum(len(word) for word in words if word.isalnum())\n",
    "        processed += 1\n",
    "        \n",
    "        # Determine question type based on keywords\n",
    "        for word in words[:4]:  # Check first few words for question type indicators\n",
    "            if word in factual_keywords:\n",
    "                metrics[\"question_types\"][\"factual\"] += 1\n",
    "                break\n",
    "            elif word in conceptual_keywords:\n",
    "                metrics[\"question_types\"][\"conceptual\"] += 1\n",
    "                break\n",
    "            elif word in analytical_keywords:\n",
    "                metrics[\"question_types\"][\"analytical\"] += 1\n",
    "                break\n",
    "    \n",
    "    # Calculate averages\n",
    "    if processed > 0:\n",
    "        metrics[\"avg_word_count\"] = total_words / processed\n",
    "        metrics[\"avg_sentence_count\"] = total_sentences / processed\n",
    "        if total_words > 0:\n",
    "            metrics[\"avg_word_length\"] = total_chars / total_words\n",
    "    \n",
    "        # Calculate Flesch Reading Ease score (simplified)\n",
    "        if total_sentences > 0:\n",
    "            words_per_sentence = total_words / total_sentences\n",
    "            syllables_per_word = total_chars / total_words / 3  # Rough approximation\n",
    "            \n",
    "            metrics[\"readability_scores\"][\"flesch_reading_ease\"] = 206.835 - (1.015 * words_per_sentence) - (84.6 * syllables_per_word)\n",
    "            metrics[\"readability_scores\"][\"flesch_kincaid_grade\"] = (0.39 * words_per_sentence) + (11.8 * syllables_per_word) - 15.59\n",
    "    \n",
    "    # Convert question type counts to percentages\n",
    "    total_classified = sum(metrics[\"question_types\"].values())\n",
    "    if total_classified > 0:\n",
    "        for qtype in metrics[\"question_types\"]:\n",
    "            metrics[\"question_types\"][qtype] = (metrics[\"question_types\"][qtype] / total_classified) * 100\n",
    "    \n",
    "    return metrics"
   ],
   "id": "945b412e9f728666",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:06:22.194637Z",
     "start_time": "2025-04-25T22:06:22.188913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_interpretation(quality_score):\n",
    "    if quality_score >= 0.85:\n",
    "        return \"Excellent: Questions are of high quality across all dimensions\"\n",
    "    elif quality_score >= 0.75:\n",
    "        return \"Very Good: Questions are effective with minor areas for improvement\"\n",
    "    elif quality_score >= 0.65:\n",
    "        return \"Good: Questions meet basic requirements but would benefit from refinement\"\n",
    "    elif quality_score >= 0.55:\n",
    "        return \"Acceptable: Questions are usable but have significant room for improvement\"\n",
    "    elif quality_score >= 0.45:\n",
    "        return \"Needs Improvement: Questions have issues that should be addressed\"\n",
    "    else:\n",
    "        return \"Poor: Questions require major revision across multiple dimensions\"\n",
    "\n"
   ],
   "id": "1ce567a1dcf0a67d",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:06:22.295629Z",
     "start_time": "2025-04-25T22:06:22.279250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_comprehensive_evaluation(topic, course, use_lstm=True):\n",
    "    # Step 1: Generate questions with your RAG system\n",
    "    rag_questions = generate_quiz(\n",
    "        topic=topic,\n",
    "        course=course,\n",
    "        num_questions=5,\n",
    "        question_types=[\"mcq\", \"true_false\", \"short_answer\"]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate reference questions\n",
    "    reference_questions = generate_generic_quiz(\n",
    "        topic=topic,\n",
    "        course=course,\n",
    "        num_questions=5,\n",
    "        question_types=[\"mcq\", \"true_false\", \"short_answer\"]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Get the source contexts used\n",
    "    source_contexts = [q.get(\"source_chunk\", {\"text\": \"\"}) for q in rag_questions]\n",
    "    \n",
    "    # Step 4: Run evaluation metrics\n",
    "    results = {\n",
    "        \"basic_metrics\": evaluate_quiz_quality(rag_questions),\n",
    "        \"rouge_metrics\": evaluate_with_rouge(rag_questions, reference_questions),\n",
    "        \"performance_metrics\": benchmark_performance(topic, course)\n",
    "    }\n",
    "    \n",
    "    # Step 5: Add complexity analysis\n",
    "    try:\n",
    "        results[\"complexity_metrics\"] = analyze_question_complexity(rag_questions)\n",
    "    except Exception as e:\n",
    "        print(f\"Complexity analysis failed: {e}\")\n",
    "        results[\"complexity_metrics\"] = {\"error\": str(e)}\n",
    "    \n",
    "    # Step 6: Add LSTM similarity metrics if requested\n",
    "    if use_lstm:\n",
    "        try:\n",
    "            results[\"lstm_similarity\"] = evaluate_with_lstm_similarity(\n",
    "                rag_questions,\n",
    "                reference_questions,\n",
    "                source_contexts\n",
    "            )\n",
    "            print(\"LSTM similarity metrics successfully calculated\")\n",
    "        except Exception as e:\n",
    "            print(f\"LSTM similarity evaluation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results[\"lstm_similarity\"] = {\"error\": str(e)}\n",
    "    \n",
    "    # Step 7: DeepEval metrics\n",
    "    try:\n",
    "        results[\"deepeval_metrics\"] = evaluate_with_deepeval(rag_questions, source_contexts)\n",
    "    except Exception as e:\n",
    "        print(f\"DeepEval evaluation failed: {e}\")\n",
    "        results[\"deepeval_metrics\"] = {\"error\": str(e)}\n",
    "    \n",
    "    # Step 8: LM Studio evaluation if available\n",
    "    try:\n",
    "        results[\"lm_studio_evaluation\"] = evaluate_with_lm_studio(\n",
    "            rag_questions, \n",
    "            api_url=\"http://localhost:4500/v1/completions\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"LM Studio evaluation failed: {e}\")\n",
    "        results[\"lm_studio_evaluation\"] = {\"error\": str(e)}\n",
    "    \n",
    "    # Calculate an aggregate quality score (weighted average of various metrics)\n",
    "    weights = {\n",
    "        \"format_compliance\": 0.15,\n",
    "        \"rouge\": 0.05,\n",
    "        \"context_similarity\": 0.2,\n",
    "        \"diversity\": 0.15,\n",
    "        \"answer_relevancy\": 0.3,\n",
    "        \"complexity\": 0.15\n",
    "    }\n",
    "    \n",
    "    quality_score = 0\n",
    "    quality_components = {}\n",
    "    \n",
    "    # Basic format compliance\n",
    "    if \"basic_metrics\" in results:\n",
    "        format_score = results[\"basic_metrics\"].get(\"format_compliance\", 0)\n",
    "        quality_components[\"format_compliance\"] = format_score\n",
    "        quality_score += format_score * weights[\"format_compliance\"]\n",
    "    \n",
    "    # ROUGE-L score (lexical similarity)\n",
    "    if \"rouge_metrics\" in results:\n",
    "        rouge_score = results[\"rouge_metrics\"].get(\"rougeL\", 0)\n",
    "        quality_components[\"rouge\"] = rouge_score\n",
    "        quality_score += rouge_score * weights[\"rouge\"]\n",
    "    \n",
    "    # LSTM similarity to context\n",
    "    if \"lstm_similarity\" in results and isinstance(results[\"lstm_similarity\"], dict):\n",
    "        lstm = results[\"lstm_similarity\"]\n",
    "        if \"avg_question_to_context_similarity\" in lstm:\n",
    "            context_sim = lstm[\"avg_question_to_context_similarity\"] / 100  # Convert to 0-1 scale\n",
    "            quality_components[\"context_similarity\"] = context_sim\n",
    "            quality_score += context_sim * weights[\"context_similarity\"]\n",
    "        if \"diversity_score\" in lstm:\n",
    "            diversity = lstm[\"diversity_score\"] / 100  # Convert to 0-1 scale\n",
    "            quality_components[\"diversity\"] = diversity\n",
    "            quality_score += diversity * weights[\"diversity\"]\n",
    "    \n",
    "    # Answer relevancy from DeepEval\n",
    "    if \"deepeval_metrics\" in results:\n",
    "        de = results[\"deepeval_metrics\"]\n",
    "        if \"answer_relevancy\" in de:\n",
    "            relevancy = de[\"answer_relevancy\"]\n",
    "            quality_components[\"answer_relevancy\"] = relevancy\n",
    "            quality_score += relevancy * weights[\"answer_relevancy\"]\n",
    "    \n",
    "    if \"complexity_metrics\" in results:\n",
    "        complexity = results[\"complexity_metrics\"]\n",
    "        if \"question_types\" in complexity:\n",
    "            import math\n",
    "            types = complexity[\"question_types\"]\n",
    "            if any(types.values()):\n",
    "                entropy = 0\n",
    "                for v in types.values():\n",
    "                    if v > 0:\n",
    "                        p = v / 100  \n",
    "                        entropy -= p * math.log2(p)\n",
    "                normalized_entropy = min(entropy / 1.585, 1.0)\n",
    "                quality_components[\"complexity\"] = normalized_entropy\n",
    "                quality_score += normalized_entropy * weights[\"complexity\"]\n",
    "    \n",
    "    results[\"quality_score\"] = quality_score\n",
    "    results[\"quality_components\"] = quality_components\n",
    "    \n",
    "    # Step 9: Save and display results\n",
    "    print(f\"\\n=== Comprehensive Evaluation Results for {topic} ===\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    print(f\"Format Compliance: {results['basic_metrics'].get('format_compliance', 0):.2f}\")\n",
    "    \n",
    "    # ROUGE metrics\n",
    "    print(f\"ROUGE-L F1 Score: {results['rouge_metrics'].get('rougeL', 0):.4f}\")\n",
    "    \n",
    "    # Complexity metrics\n",
    "    if \"complexity_metrics\" in results and isinstance(results[\"complexity_metrics\"], dict):\n",
    "        complexity = results[\"complexity_metrics\"]\n",
    "        if \"question_types\" in complexity:\n",
    "            print(\"\\nQuestion Type Distribution:\")\n",
    "            for qtype, percentage in complexity[\"question_types\"].items():\n",
    "                print(f\"- {qtype}: {percentage:.1f}%\")\n",
    "        if \"readability_scores\" in complexity:\n",
    "            print(f\"Readability: {complexity['readability_scores'].get('flesch_reading_ease', 'N/A'):.1f}\")\n",
    "            print(f\"Avg. Word Count: {complexity.get('avg_word_count', 'N/A'):.1f}\")\n",
    "    \n",
    "    if \"lstm_similarity\" in results and isinstance(results[\"lstm_similarity\"], dict):\n",
    "        lstm = results[\"lstm_similarity\"]\n",
    "        if \"avg_question_to_context_similarity\" in lstm:\n",
    "            print(f\"\\nLSTM Context Similarity: {lstm['avg_question_to_context_similarity']:.2f}%\")\n",
    "        if \"avg_question_to_reference_similarity\" in lstm:\n",
    "            print(f\"LSTM Reference Similarity: {lstm['avg_question_to_reference_similarity']:.2f}%\")\n",
    "        if \"diversity_score\" in lstm:\n",
    "            print(f\"LSTM Question Diversity: {lstm['diversity_score']:.2f}%\")\n",
    "    \n",
    "    # DeepEval metrics\n",
    "    if \"deepeval_metrics\" in results:\n",
    "        de = results[\"deepeval_metrics\"]\n",
    "        if \"answer_relevancy\" in de:\n",
    "            print(f\"\\nDeepEval Answer Relevancy: {de['answer_relevancy']:.2f}\")\n",
    "            print(f\"Questions Evaluated: {de.get('num_evaluated', 0)}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    print(f\"\\nGeneration Speed: {results['performance_metrics'].get('questions_per_minute', 0):.2f} questions/minute\")\n",
    "    \n",
    "    # Overall quality score\n",
    "    print(f\"\\nOverall Quality Score: {quality_score:.2f} / 1.0\")\n",
    "    if quality_components:\n",
    "        print(\"Quality Component Scores:\")\n",
    "        for component, score in quality_components.items():\n",
    "            print(f\"- {component}: {score:.2f}\")\n",
    "    \n",
    "    return results"
   ],
   "id": "6241235092266b33",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:12:39.155418Z",
     "start_time": "2025-04-25T22:06:22.357690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_comprehensive_evaluation(\n",
    "    topic=\"Artificial Intelligence\",\n",
    "    course=\"Artificial Intelligence\"\n",
    ")"
   ],
   "id": "c3268f5b19db58a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for chunks related to: Artificial Intelligence\n",
      "Found 7 usable chunks out of 10 total chunks\n",
      "Generating mcq question 1/5\n",
      "Prompt prepared for mcq question (length: 1161 chars)\n",
      "Token count: 306 tokens\n",
      "Question 1 generated in 14.56 seconds\n",
      "Generating true_false question 2/5\n",
      "Prompt prepared for true_false question (length: 1288 chars)\n",
      "Token count: 275 tokens\n",
      "Question 2 generated in 6.97 seconds\n",
      "Generating short_answer question 3/5\n",
      "Prompt prepared for short_answer question (length: 916 chars)\n",
      "Token count: 201 tokens\n",
      "Question 3 generated in 1.68 seconds\n",
      "Generating mcq question 4/5\n",
      "Prompt prepared for mcq question (length: 1273 chars)\n",
      "Token count: 301 tokens\n",
      "Question 4 generated in 17.54 seconds\n",
      "Generating true_false question 5/5\n",
      "Prompt prepared for true_false question (length: 1341 chars)\n",
      "Token count: 280 tokens\n",
      "Question 5 generated in 2.77 seconds\n",
      "\n",
      "Quiz generation completed:\n",
      "Total time: 45.64 seconds\n",
      "Successfully generated 5/5 questions\n",
      "Generated Questions:\n",
      " \"\"\"\n",
      "    # Define the quiz\n",
      "    quiz = {\n",
      "        \"name\": \"Artificial Intelligence Quiz\",\n",
      "        \"description\": \"Test your knowledge on Artificial Intelligence\",\n",
      "        \"questions\": [\n",
      "            {\n",
      "                \"type\": \"mcq\",\n",
      "                \"question\": \"What is the primary goal of machine learning?\",\n",
      "                \"options\": [\"To make decisions based on data\", \"To solve complex problems\", \"To learn from experience\"],\n",
      "                \"correct_answer\": 1,\n",
      "                \"difficulty\": 3\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"true_false\",\n",
      "                \"question\": \"Is Artificial Intelligence a subset of Machine Learning?\",\n",
      "                \"correct_answer\": False,\n",
      "                \"difficulty\": 2\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"short_answer\",\n",
      "                \"question\": \"What is the term for a machine that can think and learn like a human being?\",\n",
      "                \"answer\": \"Artificial General Intelligence\",\n",
      "                \"difficulty\": 3\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"mcq\",\n",
      "                \"question\": \"What is the term for a type of machine learning where an algorithm learns from experience and improves its performance over time?\",\n",
      "                \"options\": [\"Supervised Learning\", \"Unsupervised Learning\", \"Reinforcement Learning\"],\n",
      "                \"correct_answer\": 2,\n",
      "                \"difficulty\": 3\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"short_answer\",\n",
      "                \"question\": \"What is the term for a type of Artificial Intelligence that uses natural language processing to understand and generate human-like text?\",\n",
      "                \"answer\": \"Natural Language Processing (NLP)\",\n",
      "                \"difficulty\": 3\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "\n",
      "    # Print the quiz\n",
      "    print(json.dumps(quiz, indent=4))\n",
      "\n",
      "# Call the function\n",
      "create_quiz()```\n",
      "\n",
      "This code defines a dictionary `quiz` that contains information about the quiz, including its name, description, and questions. Each question is represented as a dictionary with attributes such as type (mcq, true_false, or short_answer), question text, options for multiple-choice questions, correct answer, and difficulty level.\n",
      "\n",
      "The `create_quiz()` function prints the quiz in JSON format using the `json.dumps()` method. The `indent=4` parameter is used to pretty-print the JSON output with an indentation of 4 spaces.\n",
      "\n",
      "You can modify this code to suit your specific needs by adding or removing questions, changing question types, and adjusting difficulty levels. You can also use this code as a\n",
      "Prompt prepared for mcq question (length: 1161 chars)\n",
      "Token count: 306 tokens\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "LSTM similarity metrics successfully calculated\n",
      "Using AnswerRelevancyMetric\n",
      "ContextualRelevanceMetric not available in your DeepEval installation\n",
      "FactualConsistencyMetric not available in your DeepEval installation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:21, 21.65s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.8, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 0.80 because the statement explicitly stating that Predictive Analytics is NOT an area commonly associated with artificial intelligence is irrelevant to the topic of Introduction to Artificial Intelligence, preventing the score from being higher., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7. Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to gener...\n",
      "  - actual output: ** Which of the following is NOT an area commonly associated with artificial intelligence?\n",
      "\n",
      "A.  Natural Language Processing\n",
      "B. ** Predictive Analytics**\n",
      "C. ** Image Recognition**\n",
      "D. ** Robotics & Automation **\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:10, 10.70s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 1.00 because the response is highly relevant to the input, addressing the concept of superintelligence directly and providing a clear connection to the topic of artificial general intelligence., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced s...\n",
      "  - actual output: ** A superintelligent AI could potentially self-improve indefinitely without any external input from humans.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:16, 16.10s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Answer Relevancy (score: 0.5, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 0.50 because it's dragged down by an irrelevant statement about a prompt or instruction, which doesn't contribute to discussing AI or related topics., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by...\n",
      "  - actual output: Let me know when you're ready for the AI Question!  😊\n",
      "Answer: [Missing - please determine from context]\n",
      "Explanation: Answer cannot be determined from the generated content.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:16, 16.59s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.8, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 0.80 because the actual output mentions precision farming techniques, which are not relevant to high-profile applications of AI discussed in the input., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Si...\n",
      "  - actual output: ** Which high-tech application represents an example of how Artificial Intelligence has impacted daily life today?\n",
      "\n",
      "A.  Automated car manufacturing lines\n",
      "B. ** Generative content creation platforms such as Midjourney or Dall-E 2**,\n",
      "C. ** Improved agricultural practices through precision farming techniques,**\n",
      "D. ** Social media algorithms used for targeted advertising\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:14, 14.26s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 1.00 because the actual output perfectly addresses and responds to all aspects of the input, making it highly relevant., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, ...\n",
      "  - actual output: **  Machine Learning algorithms are an integral part of artificial intelligence (IA).\n",
      "Answer : [True/False]\n",
      "Explaination: [Briefly explain why.]\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comprehensive Evaluation Results for Artificial Intelligence ===\n",
      "Format Compliance: 0.81\n",
      "ROUGE-L F1 Score: 0.0270\n",
      "\n",
      "Question Type Distribution:\n",
      "- factual: 100.0%\n",
      "- conceptual: 0.0%\n",
      "- analytical: 0.0%\n",
      "Readability: 78.6\n",
      "Avg. Word Count: 37.2\n",
      "\n",
      "LSTM Context Similarity: 3.11%\n",
      "LSTM Reference Similarity: 1.19%\n",
      "LSTM Question Diversity: 94.92%\n",
      "\n",
      "DeepEval Answer Relevancy: 0.82\n",
      "Questions Evaluated: 5\n",
      "\n",
      "Generation Speed: 49.31 questions/minute\n",
      "\n",
      "Overall Quality Score: 0.52 / 1.0\n",
      "Quality Component Scores:\n",
      "- format_compliance: 0.81\n",
      "- rouge: 0.03\n",
      "- context_similarity: 0.03\n",
      "- diversity: 0.95\n",
      "- answer_relevancy: 0.82\n",
      "- complexity: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'basic_metrics': {'format_compliance': 0.8095238095238095,\n",
       "  'content_relevance': 0,\n",
       "  'question_clarity': 0,\n",
       "  'overall_quality': 0},\n",
       " 'rouge_metrics': {'rouge1': 0.027022780832678717,\n",
       "  'rouge2': 0.01414141414141414,\n",
       "  'rougeL': 0.027022780832678717},\n",
       " 'performance_metrics': {'retrieval_time': 0.0464625358581543,\n",
       "  'generation_time': 1.216700792312622,\n",
       "  'total_time': 1.2631633281707764,\n",
       "  'tokens_per_second': 0,\n",
       "  'questions_per_minute': 49.31368531942523},\n",
       " 'complexity_metrics': {'avg_word_count': 37.2,\n",
       "  'avg_sentence_count': 2.6,\n",
       "  'avg_word_length': 4.032258064516129,\n",
       "  'readability_scores': {'flesch_reading_ease': 78.6030148883375,\n",
       "   'flesch_kincaid_grade': 5.8502150537634385},\n",
       "  'question_types': {'factual': 100.0, 'conceptual': 0.0, 'analytical': 0.0}},\n",
       " 'lstm_similarity': {'question_to_reference_similarity': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.9607843137254901,\n",
       "   4.0],\n",
       "  'question_to_context_similarity': [2.2222222222222223,\n",
       "   2.564102564102564,\n",
       "   1.8518518518518516,\n",
       "   1.4084507042253522,\n",
       "   7.5],\n",
       "  'question_to_question_similarity_matrix': [[100.0,\n",
       "    2.4390243902439024,\n",
       "    1.9230769230769231,\n",
       "    13.432835820895523,\n",
       "    9.30232558139535],\n",
       "   [2.4390243902439024, 100.0, 5.405405405405405, 1.639344262295082, 3.125],\n",
       "   [1.9230769230769231,\n",
       "    5.405405405405405,\n",
       "    100.0,\n",
       "    1.3888888888888888,\n",
       "    2.3255813953488373],\n",
       "   [13.432835820895523,\n",
       "    1.639344262295082,\n",
       "    1.3888888888888888,\n",
       "    100.0,\n",
       "    9.836065573770492],\n",
       "   [9.30232558139535, 3.125, 2.3255813953488373, 9.836065573770492, 100.0]],\n",
       "  'diversity_score': 94.91824517586797,\n",
       "  'avg_question_to_reference_similarity': 1.192156862745098,\n",
       "  'avg_question_to_context_similarity': 3.109325468480398},\n",
       " 'deepeval_metrics': {'answer_relevancy': 0.82,\n",
       "  'num_evaluated': 5,\n",
       "  'evaluations': [{'metric': 'answer_relevancy',\n",
       "    'score': 0.8,\n",
       "    'reason': 'The score is 0.80 because the statement explicitly stating that Predictive Analytics is NOT an area commonly associated with artificial intelligence is irrelevant to the topic of Introduction to Artificial Intelligence, preventing the score from being higher.',\n",
       "    'question_index': 0},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 1.0,\n",
       "    'reason': 'The score is 1.00 because the response is highly relevant to the input, addressing the concept of superintelligence directly and providing a clear connection to the topic of artificial general intelligence.',\n",
       "    'question_index': 1},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 0.5,\n",
       "    'reason': \"The score is 0.50 because it's dragged down by an irrelevant statement about a prompt or instruction, which doesn't contribute to discussing AI or related topics.\",\n",
       "    'question_index': 2},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 0.8,\n",
       "    'reason': 'The score is 0.80 because the actual output mentions precision farming techniques, which are not relevant to high-profile applications of AI discussed in the input.',\n",
       "    'question_index': 3},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 1.0,\n",
       "    'reason': 'The score is 1.00 because the actual output perfectly addresses and responds to all aspects of the input, making it highly relevant.',\n",
       "    'question_index': 4}]},\n",
       " 'lm_studio_evaluation': [{'question': '**Question:** Which of the following is NOT an area commonly associated with artificial intelligence...',\n",
       "   'full_evaluation': \"\\n\\n\\n\\n**Clarity:** 9/10. The question clearly states what is being asked, which area is NOT commonly associated with artificial intelligence. However, the answer choices could be more concise to avoid confusion.\\n\\n\\n\\n**Relevance:** 8/10. While all options are related to AI in some way, Predictive Analytics might not be as directly associated with AI as other areas like Natural Language Processing or Image Recognition. However, it is still a relevant topic in the field of AI.\\n\\n\\n\\n**Educational Value:** 9/10. This question requires critical thinking and analysis of the subject matter, making it an excellent tool for assessing students' understanding of AI domains. It also encourages them to think about what areas are not typically associated with AI.\\n\\n\\n\\n**Technical Accuracy:** 8/10. The statement that Predictive Analytics is primarily statistical methods used to analyze trends rather than cognitive processes or machine learning techniques might be slightly misleading, as some predictive analytics models do involve machine learning. However, it's generally accurate and serves the purpose of this question.\\n\\n\\n\\nI hope you find my ratings helpful! Let me know if you have any further questions or need more information.  I'm happy to assist you with your evaluation needs.\",\n",
       "   'ratings': {}},\n",
       "  {'question': '**Question:** A superintelligent AI could potentially self-improve indefinitely without any external...',\n",
       "   'full_evaluation': ' */\\n\\n# Define the ratings for each criterion\\nclarity = 8\\nrelevance = 9\\neducational_value = 7\\ntechnical_accuracy = 6\\n\\n# Print the ratings and explanations\\nprint(\"Clarity:\", clarity, \"/10\")\\nprint(\"Explanation: The question is clear in its wording but assumes a level of prior knowledge about superintelligent AI.\")\\n\\nprint(\"\\\\nRelevance:\", relevance, \"/10\")\\nprint(\"Explanation: The question is highly relevant to the subject area of artificial intelligence and superintelligence.\")\\n\\nprint(\"\\\\nEducational Value:\", educational_value, \"/10\")\\nprint(\"Explanation: The question has some educational value as it touches on a key concept in AI research but could be more effective if it prompted critical thinking or analysis.\")\\n\\nprint(\"\\\\nTechnical Accuracy:\", technical_accuracy, \"/10\")\\nprint(\"Explanation: The statement is partially correct. While superintelligent AI may have the potential to self-improve, this does not necessarily mean they can do so indefinitely without any external input from humans.\")  # Changed this line to reflect the actual rating and explanation. \\n\\n# Print a summary of the ratings\\nprint(\"\\\\nSummary:\")\\nprint(\"The question has high relevance but lower clarity, educational value, and technical accuracy.\")  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect the actual summary.  # Changed this line to reflect',\n",
       "   'ratings': {}},\n",
       "  {'question': \"Let me know when you're ready for the AI Question!  😊\\nAnswer: [Missing - please determine from conte...\",\n",
       "   'full_evaluation': '\\n\\n\\n\\nI\\'m happy to help you with evaluating this question. However, I notice that there is no actual question provided in the prompt. It seems like there\\'s a placeholder or a joke (\"Let me know when you\\'re ready for the AI Question!  😊\") instead of an actual question.\\n\\nGiven this situation, I\\'ll provide my ratings based on the assumption that the task is to evaluate the placeholder as if it were a real question. Keep in mind that these ratings are subjective and might not accurately reflect the quality of a well-crafted educational quiz question.\\n\\nHere are my ratings:\\n\\n1. Clarity: 2/10\\nThe placeholder lacks clarity, as it\\'s not a clear or specific question. It appears to be a joke or a prompt for something else.\\n2. Relevance: N/A (Not Applicable)\\nSince there is no actual question, it\\'s difficult to assess its relevance to any subject area.\\n3. Educational Value: 0/10\\nThe placeholder lacks educational value as it doesn\\'t provide any learning opportunity or insight into a particular topic.\\n4. Technical Accuracy: N/A (Not Applicable)\\nAs the content is not factual or informative, technical accuracy is also not applicable.\\n\\nPlease let me know if you\\'d like me to re-evaluate anything or provide further clarification! 😊',\n",
       "   'ratings': {'clarity': 2, 'educational_value': 0}},\n",
       "  {'question': '**Example:**  What is Machine Learning?\\nA: The process of teaching computers to perform specific tas...',\n",
       "   'full_evaluation': '\\n\\n\\n\\n**Clarity:** 9/10\\nThe question is clear in what it\\'s asking: an example of how AI has impacted daily life. The options provided are also concise and directly related to the topic, making it easy to understand what the respondent is being asked.\\n\\n**Relevance:** 8/10\\nWhile all options relate to technology or its applications, some may not be as relevant to everyday life as others (e.g., automated car manufacturing lines). However, the question does ask for an example of AI\\'s impact on daily life, making most options somewhat relevant.\\n\\n**Educational Value:** 9/10\\nThis question encourages critical thinking about how technology has become integrated into our lives. It also requires understanding of what constitutes AI and its applications, which is a valuable learning objective.\\n\\n**Technical Accuracy:** 8/10\\nWhile the provided explanation for option B (Generative content creation platforms) is accurate in describing an impact of AI on daily life, it\\'s essential to note that other options might not be entirely incorrect but less directly related. This could lead to some confusion or debate about what constitutes \"daily life.\" However, all options are grounded in real-world applications of technology influenced by AI.\\n\\n\\n\\n**Final Evaluation:** 8.5/10\\nThis question is well-crafted for assessing understanding and application of AI concepts. It encourages critical thinking and requires respondents to consider how technology has become a part of our daily lives. While there might be some room for improvement in terms of technical accuracy, the question effectively covers relevant aspects of AI\\'s impact on modern life.\\n\\n**Recommendations:** Consider adding more context or clarifying language to ensure that all options are equally relevant and directly related to AI\\'s impact on daily life. This could help avoid potential confusion about what constitutes \"daily life.\" Additionally, providing a clear explanation for why option B is the correct answer might enhance educational value. Overall, this question is well-suited for assessing understanding of AI concepts in an applied context.\\n\\n**Additional Comments:** The use of specific examples like Midjourney or Dall-E 2 helps make the question more engaging and relevant to current technological advancements. This could encourage respondents to think critically about how AI is shaping our world today. However, it\\'s also worth considering including a broader range of options to ensure that the question remains fair and inclusive for all respondents.\\n\\n**Final Assessment:** Overall, this question effectively assesses understanding and application of AI concepts in an applied context. With some',\n",
       "   'ratings': {}},\n",
       "  {'question': '**Question:**  Machine Learning algorithms are an integral part of artificial intelligence (IA).\\nAns...',\n",
       "   'full_evaluation': '\\n\\n\\n\\n**Rating:** 8\\n**Explanation:**\\n\\n1. **Clarity:** 9/10 - The question is clear in its intention to test whether Machine Learning algorithms are part of Artificial Intelligence (IA). However, it could be more precise with the acronym \"AI\" instead of \"IA\", which might cause minor confusion for some readers.\\n\\n2. **Relevance:** 8/10 - This question is relevant to both Machine Learning and Artificial Intelligence, two significant areas in computer science. It touches on a fundamental concept that bridges these two fields, making it relevant but not overly specific or narrow.\\n\\n3. **Educational Value:** 9/10 - Understanding the relationship between Machine Learning algorithms and Artificial Intelligence is crucial for anyone studying or working in these fields. This question assesses basic knowledge that is foundational to more advanced topics in both areas.\\n\\n4. **Technical Accuracy:** 10/10 - The statement that Machine Learning algorithms are an integral part of Artificial Intelligence is accurate. Machine Learning is indeed a key component of AI, as it enables systems to learn from data and improve their performance over time without being explicitly programmed for each task. This makes the question technically correct.\\n\\n**Note:** Given the format provided, I\\'ve had to infer some aspects of the evaluation based on common practices in educational quiz questions and the subject matter at hand. The ratings could slightly vary depending on specific expectations or standards for such evaluations.',\n",
       "   'ratings': {}}],\n",
       " 'quality_score': 0.5173757291709681,\n",
       " 'quality_components': {'format_compliance': 0.8095238095238095,\n",
       "  'rouge': 0.027022780832678717,\n",
       "  'context_similarity': 0.03109325468480398,\n",
       "  'diversity': 0.9491824517586797,\n",
       "  'answer_relevancy': 0.82,\n",
       "  'complexity': 0.0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:19:18.409022Z",
     "start_time": "2025-04-25T22:12:39.445702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_comprehensive_evaluation(\n",
    "    topic=\"Baze de date\",\n",
    "    course=\"Baze de date\",\n",
    ")"
   ],
   "id": "1416e5a394971bc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for chunks related to: Baze de date\n",
      "Found 8 usable chunks out of 10 total chunks\n",
      "Generating mcq question 1/5\n",
      "Prompt prepared for mcq question (length: 1357 chars)\n",
      "Token count: 338 tokens\n",
      "Question 1 generated in 10.40 seconds\n",
      "Generating true_false question 2/5\n",
      "Prompt prepared for true_false question (length: 1337 chars)\n",
      "Token count: 322 tokens\n",
      "Question 2 generated in 8.43 seconds\n",
      "Generating short_answer question 3/5\n",
      "Prompt prepared for short_answer question (length: 1061 chars)\n",
      "Token count: 266 tokens\n",
      "Question 3 generated in 2.95 seconds\n",
      "Generating mcq question 4/5\n",
      "Prompt prepared for mcq question (length: 1330 chars)\n",
      "Token count: 334 tokens\n",
      "Question 4 generated in 8.51 seconds\n",
      "Generating true_false question 5/5\n",
      "Prompt prepared for true_false question (length: 1256 chars)\n",
      "Token count: 311 tokens\n",
      "Question 5 generated in 0.16 seconds\n",
      "\n",
      "Quiz generation completed:\n",
      "Total time: 33.98 seconds\n",
      "Successfully generated 5/5 questions\n",
      "Generated Questions:\n",
      " \"\"\"\n",
      "    # Define the quiz\n",
      "    quiz = {\n",
      "        \"name\": \"Baze de date\",\n",
      "        \"course\": \"Baze de date\",\n",
      "        \"questions\": [\n",
      "            {\n",
      "                \"type\": \"mcq\",\n",
      "                \"question\": \"Ce type de baza de date este utilizat pentru a stoca informatii despre oameni?\",\n",
      "                \"options\": [\"Relational\", \"NoSQL\", \"Graph\"],\n",
      "                \"correct_answer\": 0\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"true_false\",\n",
      "                \"question\": \"Bazele de date sunt utilizate numai in domeniul informaticii.\",\n",
      "                \"correct_answer\": True\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"short_answer\",\n",
      "                \"question\": \"Ce este un index in baza de date?\",\n",
      "                \"hint\": \"Raspunsul trebuie sa fie scurt si precis.\"\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"mcq\",\n",
      "                \"question\": \"Care este tipul de baza de date care suporta schema dinamică?\",\n",
      "                \"options\": [\"Relational\", \"NoSQL\", \"Graph\"],\n",
      "                \"correct_answer\": 1\n",
      "            },\n",
      "            {\n",
      "                \"type\": \"short_answer\",\n",
      "                \"question\": \"Ce este un trigger in baza de date?\",\n",
      "                \"hint\": \"Raspunsul trebuie sa fie scurt si precis.\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "\n",
      "    # Print the quiz\n",
      "    print(\"Quiz:\")\n",
      "    for question in quiz[\"questions\"]:\n",
      "        if question[\"type\"] == \"mcq\":\n",
      "            print(f\"\\n{question['question']}\")\n",
      "            for i, option in enumerate(question[\"options\"]):\n",
      "                print(f\"{i+1}. {option}\")\n",
      "            print(f\"Correct answer: {question['correct_answer'] + 1}\")\n",
      "        elif question[\"type\"] == \"true_false\":\n",
      "            print(f\"\\n{question['question']} ({'True' if question['correct_answer'] else 'False'})\")\n",
      "        elif question[\"type\"] == \"short_answer\":\n",
      "            print(f\"\\n{question['question']}\")\n",
      "            if question.get(\"hint\"):\n",
      "                print(question[\"hint\"])\n",
      "\n",
      "# Execute the code\n",
      "generate_quiz()```\n",
      "\n",
      "This code defines a quiz with 5 questions and prints it out. The quiz has different types of questions (mcq, true_false, short_answer) and is designed for a course on databases.\n",
      "\n",
      "However, this code does not actually generate\n",
      "Prompt prepared for mcq question (length: 965 chars)\n",
      "Token count: 237 tokens\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "Warning: LSTM similarity service not available at http://localhost:4501/predict_similarity\n",
      "Make sure the service is running with 'uvicorn app:app --host localhost --port 4501'\n",
      "LSTM similarity metrics successfully calculated\n",
      "Using AnswerRelevancyMetric\n",
      "ContextualRelevanceMetric not available in your DeepEval installation\n",
      "FactualConsistencyMetric not available in your DeepEval installation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |          |  0% (0/1) [Time Taken: 00:00, ?test case/s]INFO:backoff:Backing off send_request(...) for 0.1s (requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:24, 24.47s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.8571428571428571, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 0.86 because the model provided some relevant information about relational databases, but also included an irrelevant statement about hierarchical databases, which detracted from its overall relevance to the input question about different types of databases., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Există multe tipuri de baze de date:\n",
      "\n",
      "Baze de date relaționale: elementele dintr-o bază de date relațională sunt organizate ca un set de tabele, cu rânduri și coloane. Tehnologia bazelor de date relaț...\n",
      "  - actual output: **  What is the primary characteristic that differentiates \"baze de data\" in general? What type of database system can offer a high degree of efficiency when accessing information structured according to specific rules or relationships? \n",
      "\n",
      "A.  Distributed databases \n",
      "B. ** Relational Databases**\n",
      "C. ** Object Oriented Database Systems**\n",
      "D. ** Hierarchical databases **\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:20, 20.74s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.8, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 0.80 because the model provided relevant information about multi-model databases, but included irrelevant details that seem to be part of an assessment or quiz, which detracted from its overall relevance., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Baze de date multi-model: combină diferite tipuri de modele de baze de date într-un singur sistem back-end integrat cu care se pot acomoda diverse tipuri de date. Baze de date pentru documente: concep...\n",
      "  - actual output: A database is used to store information about dates. \n",
      "Answer : False\n",
      "Explanation : While databases may contain data related to dates, they primarily focus on storing various types of information beyond just dates.\n",
      "\n",
      "\n",
      "## Your turn!\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:16, 16.87s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 1.00 because the actual output is highly relevant to the input, with no irrelevant statements present., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Datele sunt tranzacționate și înregistrate prin interfața bazei de date susținută de mai multe straturi de blockchain. Baza de date în sine este partajată sub forma unui registru criptat/imuabil, care...\n",
      "  - actual output: What type of database uses tables for data storage?\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:17, 17.24s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Answer Relevancy (score: 0.5, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 0.50 because the actual output includes irrelevant statements about cloud database systems not depending on personal hardware and software configurations, as well as mentioning data types handled by cloud databases, which do not address the open source aspect of the input., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Baze de date open source: au codul sursă de tip open source, cele mai cunoscute fiind cele SQL și NoSQL\n",
      "Baze de date în cloud: colectie de date, structurate sau nestructurate aflate în platformă cloud...\n",
      "  - actual output: ** What is an important characteristic of databases in the cloud?\n",
      "A .  They can only handle structured data.\n",
      "B . They must always utilize Open Source software.\n",
      "C .  Their operations depend entirely on personal hardware and software configurations.\n",
      "D .  Cloud database systems offer diverse models with varying levels of management support by providers.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "✨ You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing meta-llama-\u001B[0m\u001B[1;38;2;55;65;81m3.1\u001B[0m\u001B[38;2;55;65;81m-8b-instruct \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mLocal Model\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using meta-llama-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3.1</span><span style=\"color: #374151; text-decoration-color: #374151\">-8b-instruct </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Local Model</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:15, 15.90s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Answer Relevancy (score: 0.6666666666666666, threshold: 0.7, strict: False, evaluation model: meta-llama-3.1-8b-instruct (Local Model), reason: The score is 0.67 because the statement about not being able to determine the answer from the generated content is irrelevant to the input, which asks about database storage capabilities., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Baza de date poate fi stocată pe mai multe computere aflate în aceeași locație fizică sau aflate în diferite rețele. Baze de date NoSQL:  denumite si baze de date nerelationale, permit stocarea și ges...\n",
      "  - actual output: \n",
      "Answer: [Missing - please determine from context]\n",
      "Explanation: Answer cannot be determined from the generated content.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[38;2;5;245;141m✓\u001B[0m Tests finished 🎉! Run \u001B[1;32m'deepeval login'\u001B[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001B[38;2;106;0;255mConfident AI\u001B[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001B[36m'deepeval login'\u001B[0m in the CLI. \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comprehensive Evaluation Results for Baze de date ===\n",
      "Format Compliance: 0.70\n",
      "ROUGE-L F1 Score: 0.0000\n",
      "\n",
      "Question Type Distribution:\n",
      "- factual: 100.0%\n",
      "- conceptual: 0.0%\n",
      "- analytical: 0.0%\n",
      "Readability: 72.5\n",
      "Avg. Word Count: 40.6\n",
      "\n",
      "LSTM Context Similarity: 0.55%\n",
      "LSTM Reference Similarity: 0.00%\n",
      "LSTM Question Diversity: 93.91%\n",
      "\n",
      "DeepEval Answer Relevancy: 0.76\n",
      "Questions Evaluated: 5\n",
      "\n",
      "Generation Speed: 4.28 questions/minute\n",
      "\n",
      "Overall Quality Score: 0.48 / 1.0\n",
      "Quality Component Scores:\n",
      "- format_compliance: 0.70\n",
      "- rouge: 0.00\n",
      "- context_similarity: 0.01\n",
      "- diversity: 0.94\n",
      "- answer_relevancy: 0.76\n",
      "- complexity: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'basic_metrics': {'format_compliance': 0.6952380952380952,\n",
       "  'content_relevance': 0,\n",
       "  'question_clarity': 0,\n",
       "  'overall_quality': 0},\n",
       " 'rouge_metrics': {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},\n",
       " 'performance_metrics': {'retrieval_time': 0.5100522041320801,\n",
       "  'generation_time': 14.020061016082764,\n",
       "  'total_time': 14.530113220214844,\n",
       "  'tokens_per_second': 0,\n",
       "  'questions_per_minute': 4.27958194555448},\n",
       " 'complexity_metrics': {'avg_word_count': 40.6,\n",
       "  'avg_sentence_count': 3.8,\n",
       "  'avg_word_length': 4.379310344827586,\n",
       "  'readability_scores': {'flesch_reading_ease': 72.49397459165155,\n",
       "   'flesch_kincaid_grade': 5.802129461585},\n",
       "  'question_types': {'factual': 100.0, 'conceptual': 0.0, 'analytical': 0.0}},\n",
       " 'lstm_similarity': {'question_to_reference_similarity': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'question_to_context_similarity': [1.4492753623188406,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.2987012987012987,\n",
       "   0.0],\n",
       "  'question_to_question_similarity_matrix': [[100.0,\n",
       "    9.859154929577464,\n",
       "    8.0,\n",
       "    15.66265060240964,\n",
       "    1.694915254237288],\n",
       "   [9.859154929577464,\n",
       "    100.0,\n",
       "    7.6923076923076925,\n",
       "    9.090909090909092,\n",
       "    2.127659574468085],\n",
       "   [8.0, 7.6923076923076925, 100.0, 5.263157894736842, 0.0],\n",
       "   [15.66265060240964,\n",
       "    9.090909090909092,\n",
       "    5.263157894736842,\n",
       "    100.0,\n",
       "    1.5384615384615385],\n",
       "   [1.694915254237288, 2.127659574468085, 0.0, 1.5384615384615385, 100.0]],\n",
       "  'diversity_score': 93.90707834228924,\n",
       "  'avg_question_to_reference_similarity': 0.0,\n",
       "  'avg_question_to_context_similarity': 0.5495953322040279},\n",
       " 'deepeval_metrics': {'answer_relevancy': 0.7647619047619048,\n",
       "  'num_evaluated': 5,\n",
       "  'evaluations': [{'metric': 'answer_relevancy',\n",
       "    'score': 0.8571428571428571,\n",
       "    'reason': 'The score is 0.86 because the model provided some relevant information about relational databases, but also included an irrelevant statement about hierarchical databases, which detracted from its overall relevance to the input question about different types of databases.',\n",
       "    'question_index': 0},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 0.8,\n",
       "    'reason': 'The score is 0.80 because the model provided relevant information about multi-model databases, but included irrelevant details that seem to be part of an assessment or quiz, which detracted from its overall relevance.',\n",
       "    'question_index': 1},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 1.0,\n",
       "    'reason': 'The score is 1.00 because the actual output is highly relevant to the input, with no irrelevant statements present.',\n",
       "    'question_index': 2},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 0.5,\n",
       "    'reason': 'The score is 0.50 because the actual output includes irrelevant statements about cloud database systems not depending on personal hardware and software configurations, as well as mentioning data types handled by cloud databases, which do not address the open source aspect of the input.',\n",
       "    'question_index': 3},\n",
       "   {'metric': 'answer_relevancy',\n",
       "    'score': 0.6666666666666666,\n",
       "    'reason': 'The score is 0.67 because the statement about not being able to determine the answer from the generated content is irrelevant to the input, which asks about database storage capabilities.',\n",
       "    'question_index': 4}]},\n",
       " 'lm_studio_evaluation': [{'question': '**Question:**  What is the primary characteristic that differentiates \"baze de data\" in general? Wha...',\n",
       "   'full_evaluation': '\\n\\n\\n\\n**Rating:** 8/10\\n\\n**Explanation:**\\n\\n1. **Clarity:** 9/10 - The question is clear in its intent to identify the primary characteristic that differentiates \"baze de data\" (which seems to be a typo or misspelling of \"database\") and asks for the type of database system that offers high efficiency when accessing information structured according to specific rules or relationships. However, the term \"baze de data\" might confuse some readers due to its unclear meaning.\\n\\n2. **Relevance:** 8/10 - The question is relevant to the subject area of databases, specifically focusing on the characteristics and types of database systems. It requires an understanding of how different types of databases organize their data, which is a key concept in computer science and information technology.\\n\\n3. **Educational Value:** 9/10 - This question has high educational value as it tests the understanding of database concepts, particularly the differences between various types of databases (relational, distributed, object-oriented, hierarchical). It encourages learners to think critically about how data is structured and accessed in different systems.\\n\\n4. **Technical Accuracy:** 8/10 - The content of the question appears to be factually correct regarding the characteristics of relational databases and their efficiency in accessing information structured according to specific rules or relationships. However, the term \"baze de data\" might introduce a minor error due to its unclear meaning, suggesting it could be a typo for \"database.\" This reduces the technical accuracy rating slightly.\\n\\n**Rating:** 8/10\\n\\n**Explanation:**\\n\\n1. **Clarity:** 9/10 - The question is clear in its intent to identify the primary characteristic that differentiates \"baze de data\" (which seems to be a typo or misspelling of \"database\") and asks for the type of database system that offers high efficiency when accessing information structured according to specific rules or relationships. However, the term \"baze de data\" might confuse some readers due to its unclear meaning.\\n\\n2. **Relevance:** 8/10 - The question is relevant to the subject area of databases, specifically focusing on the characteristics and types of database systems. It requires an understanding of how different types of databases organize their data, which is a key concept in computer science and information technology.\\n\\n3. **Educational Value:** 9/10 - This question has high educational value as it tests the understanding of database concepts, particularly the differences between various',\n",
       "   'ratings': {}},\n",
       "  {'question': '**Example:** Question:  A database is used to store information about dates. \\nAnswer : False\\nExplana...',\n",
       "   'full_evaluation': '\\n\\n\\n\\nRating 1: Clarity\\nRating 2: Relevance\\nRating 3: Educational Value\\nRating 4: Technical Accuracy\\n\\nPlease provide the ratings and explanations in this format:\\n\\n**Clarity:** [Rating] - [Explanation]\\n**Relevance:** [Rating] - [Explanation]\\n**Educational Value:** [Rating] - [Explanation]\\n**Technical Accuracy:** [Rating] - [Explanation]\\n\\nFor example:\\n**Clarity:** 8 - The question is clear and concise, but the wording could be improved for better understanding.\\n**Relevance:** 9 - The question is highly relevant to the subject area of database management.\\n**Educational Value:** 7 - The question has some educational value, but it may not challenge learners with more complex concepts.\\n**Technical Accuracy:** 6 - The content contains some factual errors or inaccuracies.   \\n\\n\\n\\n**Clarity:** 9 - The question is clear and concise.\\n**Relevance:** 8 - The question is relevant to the subject area of database management, but could be more specific.\\n**Educational Value:** 7 - The question has some educational value, but it may not challenge learners with more complex concepts.\\n**Technical Accuracy:** 5 - The content contains factual errors or inaccuracies.   \\n\\n\\n\\nThe final answer is: $\\\\boxed{5}$',\n",
       "   'ratings': {}},\n",
       "  {'question': '**Your turn:**\\n\\nQuestion: What type of database uses tables for data storage? \\nAnswer: Relational\\nEx...',\n",
       "   'full_evaluation': '\\n\\n\\n\\nRating 1: Clarity = 9/10\\nThe question is clear and concise, asking about the type of database that uses tables for storage. However, it could be even clearer if it specified what kind of \"tables\" are being referred to (e.g., relational tables). The only reason I didn\\'t give it a perfect score is because of this minor ambiguity.\\n\\nRating 2: Relevance = 10/10\\nThe question directly pertains to the subject area of databases and data storage, making it highly relevant. It\\'s an essential concept in database management systems.\\n\\nRating 3: Educational Value = 9/10\\nThis question has significant educational value as it assesses understanding of a fundamental aspect of relational databases. It encourages learners to think about how data is organized and stored within these systems. However, the explanation provided doesn\\'t add much depth or complexity to the topic, so I wouldn\\'t give it a perfect score.\\n\\nRating 4: Technical Accuracy = 10/10\\nThe answer \"Relational\" accurately describes the type of database that uses tables for data storage. The explanation correctly identifies relational databases as an example where data is stored in tables, further reinforcing the accuracy of the response. There\\'s no factual error or misinformation presented.',\n",
       "   'ratings': {}},\n",
       "  {'question': '**Question:** What is an important characteristic of databases in the cloud?\\nA .  They can only hand...',\n",
       "   'full_evaluation': \" - Clarity: 9/10\\nThe question is clear in its intent to ask about an important characteristic of databases in the cloud. However, it could be improved by specifying what kind of characteristics are being referred to (e.g., scalability, security, data management). The options provided also contribute to the clarity of the question.\\n     - Relevance: 9/10\\nThe question is highly relevant to the subject area of database systems and cloud computing. It touches on a key aspect of cloud databases that students should be aware of, making it an excellent learning opportunity.\\n     - Educational Value: 8/10\\nThis question has significant educational value as it helps learners understand the fundamental characteristics of cloud databases. However, it could be even more valuable if it prompted critical thinking or analysis rather than simply selecting from multiple-choice options.\\n     - Technical Accuracy: 9/10\\nThe correct answer (D) accurately reflects a key characteristic of cloud database systems. The other options contain inaccuracies or misleading information, making them incorrect choices. Overall, the question is well-crafted and effectively assesses learners' understanding of cloud databases.    |\\n\\nI would rate this question as follows:\\n\\n- Clarity: 9/10\\nThe question is clear in its intent to ask about an important characteristic of databases in the cloud. However, it could be improved by specifying what kind of characteristics are being referred to (e.g., scalability, security, data management). The options provided also contribute to the clarity of the question.\\n\\n- Relevance: 9/10\\nThe question is highly relevant to the subject area of database systems and cloud computing. It touches on a key aspect of cloud databases that students should be aware of, making it an excellent learning opportunity.\\n\\n- Educational Value: 8/10\\nThis question has significant educational value as it helps learners understand the fundamental characteristics of cloud databases. However, it could be even more valuable if it prompted critical thinking or analysis rather than simply selecting from multiple-choice options.\\n\\n- Technical Accuracy: 9/10\\nThe correct answer (D) accurately reflects a key characteristic of cloud database systems. The other options contain inaccuracies or misleading information, making them incorrect choices. Overall, the question is well-crafted and effectively assesses learners' understanding of cloud databases.    |\\n\\n\\n\\nRating breakdown:\\n\\n- Clarity: 9/10\\nThe question is clear in its intent to ask about an important characteristic of databases in the cloud. However,\",\n",
       "   'ratings': {'clarity': 9,\n",
       "    'relevance': 9,\n",
       "    'educational_value': 8,\n",
       "    'technical_accuracy': 9}},\n",
       "  {'question': '\\nAnswer: [Missing - please determine from context]\\nExplanation: Answer cannot be determined from the...',\n",
       "   'full_evaluation': '\\n\\n\\n\\nI will rate the given question as follows:\\n\\nClarity: 2\\nThe question lacks clarity because it does not provide enough information to understand what is being asked. The answer \"Answer: [Missing - please determine from context]\" suggests that there might be an error or omission in the provided content.\\n\\nRelevance: N/A (Not Applicable)\\nSince the question lacks clarity, it\\'s difficult to assess its relevance to the subject area.\\n\\nEducational Value: 0\\nThe question does not provide any educational value due to its unclear nature and lack of context.\\n\\nTechnical Accuracy: 0\\nThere is no factual content in this question to evaluate for technical accuracy. The provided information suggests that there might be an error or omission, but it\\'s not possible to assess the accuracy of non-existent content. \\n\\nNote: These ratings are based on the assumption that the given text is a placeholder and does not represent a real educational quiz question. If this were a genuine question, I would expect the answer to be provided in context for evaluation purposes.  The final answer is: $\\\\boxed{2}$',\n",
       "   'ratings': {'clarity': 2,\n",
       "    'educational_value': 0,\n",
       "    'technical_accuracy': 0}}],\n",
       " 'quality_score': 0.4756740938921276,\n",
       " 'quality_components': {'format_compliance': 0.6952380952380952,\n",
       "  'rouge': 0.0,\n",
       "  'context_similarity': 0.005495953322040279,\n",
       "  'diversity': 0.9390707834228924,\n",
       "  'answer_relevancy': 0.7647619047619048,\n",
       "  'complexity': 0.0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
