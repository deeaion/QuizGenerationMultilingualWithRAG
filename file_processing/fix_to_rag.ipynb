{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PDF to RAG Processing System\n",
    "# This notebook processes PDFs for RAG systems by extracting structured content, \n",
    "# classifying sections, and adding LLM summaries while preventing hallucination"
   ],
   "id": "4d9e1cd5dc962195"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import fitz \n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from langdetect import detect, LangDetectException\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from chromadb.utils import embedding_functions\n",
    "import chromadb\n",
    "from CSMetadataExtractor import CSMetadataExtractor\n",
    "# Constants\n",
    "MIN_SECTION_LENGTH = 20  \n",
    "MIN_SIMILARITY_THRESHOLD = 0.5 \n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "caf9167f68911186"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ImprovedPDFProcessor:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        classifier_model_path: str = None,\n",
    "        llm_base_url: str = \"http://localhost:4500/v1\",\n",
    "        llm_api_key: str = \"lm-studio\",\n",
    "        llm_model: str = \"meta-llama-3.1-8b-instruct\",\n",
    "        temperature: float = 0.2\n",
    "    ):\n",
    "        self.classifier_model_path = classifier_model_path\n",
    "        self.classifier = None\n",
    "        self.tokenizer = None\n",
    "        self.id2label = None\n",
    "        \n",
    "        # Setup LLM\n",
    "        self.llm = ChatOpenAI(\n",
    "            base_url=llm_base_url,\n",
    "            api_key=llm_api_key,\n",
    "            temperature=temperature,\n",
    "            model=llm_model,\n",
    "        )\n",
    "        \n",
    "        # Initialize prompt templates\n",
    "        self.setup_prompts()\n",
    "        \n",
    "        # Load classifier if provided\n",
    "        if classifier_model_path:\n",
    "            self.load_classifier()\n",
    "            \n",
    "        print(f\"Initialized PDF processor with LLM: {llm_model}\")\n",
    "    \n",
    "    def load_classifier(self):\n",
    "        \"\"\"Load the pretrained text classifier.\"\"\"\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.classifier_model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(self.classifier_model_path)\n",
    "            self.id2label = model.config.id2label\n",
    "            self.classifier = pipeline(\n",
    "                \"text-classification\", \n",
    "                model=model, \n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "            print(f\"Loaded classifier with {len(self.id2label)} classes: {list(self.id2label.values())}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading classifier: {e}\")\n",
    "            print(\"Proceeding without classification capability\")\n",
    "    \n",
    "    def setup_prompts(self):\n",
    "        \"\"\"Setup improved prompts for different types of content.\"\"\"\n",
    "        # i am saving them like this, i tried to save them in a txt file but it was not working\n",
    "        self.general_prompt = PromptTemplate(\n",
    "            input_variables=[\"title\", \"text\", \"source\"],\n",
    "            template=\"\"\"\n",
    "    You are analyzing ACADEMIC CONTENT from a document titled: \"{title}\" from source \"{source}\".\n",
    "    \n",
    "    IMPORTANT: Use ONLY the information provided in the text below. Do NOT add any information that is not present in the text. If you don't have enough information, simply state what is available without inventing details.\n",
    "    \n",
    "    TEXT TO ANALYZE:\n",
    "    ```\n",
    "    {text}\n",
    "    ```\n",
    "    \n",
    "    Instructions:\n",
    "    1. Summarize ONLY the content provided above\n",
    "    2. Extract key concepts and terms mentioned explicitly in the text\n",
    "    3. Identify the main topic based solely on the content provided\n",
    "    4. Do not add any information that's not in the original text\n",
    "    5. If the content is incomplete or unclear, reflect that in your response - don't hallucinate details\n",
    "    \n",
    "    Format your response exactly as follows:\n",
    "    Title: <title from the document, or \"Untitled\" if unclear>\n",
    "    Content: <key points from the text, maintaining the original hierarchy and organization>\n",
    "    Summary: <brief factual summary based only on what's in the text>\n",
    "    Key Concepts: <list of key concepts explicitly mentioned, comma-separated>\n",
    "    Main Topic: <the overarching topic based only on content provided>\n",
    "    Source: <source document title>\n",
    "    \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Code-specific prompt with strict instructions\n",
    "        self.code_prompt = PromptTemplate(\n",
    "            input_variables=[\"title\", \"text\", \"source\"],\n",
    "            template=\"\"\"\n",
    "    You are analyzing CODE CONTENT from a document titled: \"{title}\" from source \"{source}\".\n",
    "    \n",
    "    IMPORTANT: Use ONLY the information provided in the text below. Do NOT add any information that is not present in the text.\n",
    "    \n",
    "    CODE TO ANALYZE:\n",
    "    ```\n",
    "    {text}\n",
    "    ```\n",
    "    \n",
    "    Instructions:\n",
    "    1. Identify the programming language ONLY if it's clearly identifiable from the code\n",
    "    2. Describe only what is visible in the code snippet\n",
    "    3. Do not invent function behaviors that aren't shown\n",
    "    4. Extract only concepts that are explicitly demonstrated\n",
    "    5. Do not add any information that's not in the original code\n",
    "    \n",
    "    Format your response exactly as follows:\n",
    "    Title: <title from the document, or \"Code Snippet\" if unclear>\n",
    "    Language: <programming language if identifiable, \"Unknown\" if unclear>\n",
    "    Description: <brief description of visible code elements ONLY>\n",
    "    Key Concepts: <list of programming concepts explicitly demonstrated, comma-separated>\n",
    "    Content: <the code with proper formatting>\n",
    "    Source: <source document title>\n",
    "    \"\"\"\n",
    "            )\n",
    "        \n",
    "        # Mathematical/theoretical content prompt\n",
    "        self.math_prompt = PromptTemplate(\n",
    "            input_variables=[\"title\", \"text\", \"source\"],\n",
    "            template=\"\"\"\n",
    "        You are analyzing MATHEMATICAL/THEORETICAL CONTENT from a document titled: \"{title}\" from source \"{source}\".\n",
    "        \n",
    "        IMPORTANT: Use ONLY the information provided in the text below. Do NOT add any information that is not present in the text.\n",
    "        \n",
    "        TEXT TO ANALYZE:\n",
    "        ```\n",
    "        {text}\n",
    "        ```\n",
    "        \n",
    "        Instructions:\n",
    "        1. Identify mathematical concepts, theorems, proofs, or algorithms described\n",
    "        2. Preserve all mathematical notation and formulas\n",
    "        3. Maintain the logical flow of the mathematical arguments\n",
    "        4. Extract only concepts that are explicitly mentioned\n",
    "        5. Do not invent or extend mathematical content that isn't present\n",
    "        \n",
    "        Format your response exactly as follows:\n",
    "        Title: <title from the document>\n",
    "        Content: <mathematical content with hierarchy and notation preserved>\n",
    "        Summary: <brief factual summary of the mathematical content>\n",
    "        Key Concepts: <list of mathematical concepts explicitly mentioned, comma-separated>\n",
    "        Main Topic: <the main mathematical topic discussed>\n",
    "        Source: <source document title>\n",
    "        \"\"\"\n",
    "        )\n",
    "    \n",
    "    def extract_structured_content(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract structured content from PDF with hierarchy preserved. - > keeping the bulletpoints\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        doc_title = os.path.basename(pdf_path)\n",
    "        \n",
    "        # First collect all blocks with their metadata\n",
    "        all_blocks = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            \n",
    "            for block in blocks:\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                \n",
    "                # Process each line separately to handle multi-line bullet points\n",
    "                for line in block[\"lines\"]:\n",
    "                    line_text = \"\"\n",
    "                    first_char = \"\"\n",
    "                    max_font_size = 0\n",
    "                    \n",
    "                    for span in line[\"spans\"]:\n",
    "                        if not first_char and span[\"text\"].strip():\n",
    "                            first_char = span[\"text\"].strip()[0]\n",
    "                        max_font_size = max(max_font_size, span[\"size\"])\n",
    "                        line_text += span[\"text\"] + \" \"\n",
    "                    \n",
    "                    line_text = line_text.strip()\n",
    "                    if not line_text:\n",
    "                        continue\n",
    "                    \n",
    "                    if (re.match(r'^[\\d]+', line_text) or \n",
    "                    re.match(r'^Page \\d+', line_text, re.IGNORECASE) or\n",
    "                    'LECTURE' in line_text and len(line_text) < 25 or\n",
    "                    re.match(r'^[\\d]+ / \\d+', line_text)):  \n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate x position (for indentation)\n",
    "                    x_pos = line[\"spans\"][0][\"bbox\"][0] if line[\"spans\"] else block[\"bbox\"][0]\n",
    "                    \n",
    "                    # Check if this is a bullet point by first character -> in the future i want to search for a better way to detect bullet points\n",
    "                    is_bullet = (first_char in \"■◦•o-*\" or \n",
    "                                line_text.lstrip().startswith((\"■\", \"◦\", \"•\", \"o\", \"-\", \"*\")))\n",
    "                    \n",
    "                    # Add to collection\n",
    "                    all_blocks.append({\n",
    "                        \"text\": line_text,\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": line[\"bbox\"],\n",
    "                        \"font_size\": max_font_size,\n",
    "                        \"x_pos\": x_pos,  #\n",
    "                        \"y_pos\": line[\"bbox\"][1], \n",
    "                        \"first_char\": first_char,\n",
    "                        \"is_bullet\": is_bullet\n",
    "                    })\n",
    "        \n",
    "        all_blocks.sort(key=lambda b: (b[\"page\"], b[\"y_pos\"]))\n",
    "        \n",
    "        font_sizes = sorted([b[\"font_size\"] for b in all_blocks if b[\"font_size\"] > 0], reverse=True)\n",
    "        main_heading_threshold = font_sizes[min(3, len(font_sizes)-1)] if font_sizes else 15\n",
    "        \n",
    "        # Process blocks to build sections\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        current_title = None\n",
    "        current_content = []\n",
    "        \n",
    "        for block in all_blocks:\n",
    "            # Determine if this is a main heading\n",
    "            is_heading = (\n",
    "                block[\"font_size\"] >= main_heading_threshold or \n",
    "                (block[\"text\"].isupper() and len(block[\"text\"]) > 3 and \n",
    "                 not block[\"text\"].startswith((\"◦\", \"o\", \"•\", \"*\", \"-\")))\n",
    "            )\n",
    "            \n",
    "            # Get indentation level based on x position\n",
    "            indent_level = 0\n",
    "            if block[\"x_pos\"] > 70:\n",
    "                indent_level = 1\n",
    "            if block[\"x_pos\"] > 130:\n",
    "                indent_level = 2\n",
    "            \n",
    "            # Format the text with appropriate indentation\n",
    "            formatted_text = \"\"\n",
    "            for _ in range(indent_level):\n",
    "                formatted_text += \"\\t\"\n",
    "            \n",
    "            # Add bullet marker if needed\n",
    "            if block[\"is_bullet\"]:\n",
    "                # Clean the text by removing the original bullet marker\n",
    "                clean_text = re.sub(r'^[\\s■◼▪◦•o\\-*]+', '', block[\"text\"])\n",
    "                formatted_text += \"- \" + clean_text\n",
    "            else:\n",
    "                formatted_text += block[\"text\"]\n",
    "            \n",
    "            # Handle section structure\n",
    "            if is_heading:\n",
    "                # Save previous section if it exists\n",
    "                if current_title and current_content:\n",
    "                    section_content = \"\\n\".join(current_content)\n",
    "                    if len(section_content) > MIN_SECTION_LENGTH:\n",
    "                        sections.append({\n",
    "                            \"title\": current_title,\n",
    "                            \"content\": section_content,\n",
    "                            \"source\": doc_title\n",
    "                        })\n",
    "                \n",
    "                # Start new section\n",
    "                current_title = block[\"text\"]\n",
    "                current_content = []\n",
    "            else:\n",
    "                # Add to current section\n",
    "                if current_title:\n",
    "                    current_content.append(formatted_text)\n",
    "                else:\n",
    "                    # If no section yet, use this as the section title\n",
    "                    current_title = block[\"text\"]\n",
    "        \n",
    "        # Add the last section\n",
    "        if current_title and current_content:\n",
    "            section_content = \"\\n\".join(current_content)\n",
    "            if len(section_content) > MIN_SECTION_LENGTH:\n",
    "                sections.append({\n",
    "                    \"title\": current_title,\n",
    "                    \"content\": section_content,\n",
    "                    \"source\": doc_title\n",
    "                })\n",
    "        \n",
    "        # If no sections were created (possibly due to unusual PDF structure),\n",
    "        # try grouping blocks by page\n",
    "        if not sections:\n",
    "            print(\"No clear sections found - grouping by page\")\n",
    "            pages_content = defaultdict(list)\n",
    "            for block in all_blocks:\n",
    "                pages_content[block[\"page\"]].append(block[\"text\"])\n",
    "            \n",
    "            for page_num, content in pages_content.items():\n",
    "                full_content = \"\\n\".join(content)\n",
    "                if len(full_content) > MIN_SECTION_LENGTH:\n",
    "                    sections.append({\n",
    "                        \"title\": f\"Page {page_num + 1}\",\n",
    "                        \"content\": full_content,\n",
    "                        \"source\": doc_title\n",
    "                    })\n",
    "                    \n",
    "        # Apply post-processing to clean up sections\n",
    "        return self.post_process_sections(sections)\n",
    "    \n",
    "    def post_process_sections(self, sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Apply post-processing to clean up sections.\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        # Merge short sections with the next one if appropriate\n",
    "        i = 0\n",
    "        while i < len(sections):\n",
    "            section = sections[i]\n",
    "            \n",
    "            # Clean title and content\n",
    "            section[\"title\"] = self.clean_text(section[\"title\"])\n",
    "            section[\"content\"] = self.clean_text(section[\"content\"])\n",
    "            \n",
    "            # Skip sections with metadata\n",
    "            if self.is_metadata(section):\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            if len(section[\"content\"]) < 200 and i + 1 < len(sections):\n",
    "                next_section = sections[i+1]\n",
    "                \n",
    "                similarity = self.calculate_similarity(section[\"content\"], next_section[\"content\"])\n",
    "                \n",
    "                if similarity > MIN_SIMILARITY_THRESHOLD:\n",
    "                    # Merge with next section\n",
    "                    merged = {\n",
    "                        \"title\": section[\"title\"],\n",
    "                        \"content\": section[\"content\"] + \"\\n\\n\" + next_section[\"content\"],\n",
    "                        \"source\": section[\"source\"]\n",
    "                    }\n",
    "                    processed.append(merged)\n",
    "                    i += 2  \n",
    "                    continue\n",
    "            \n",
    "            processed.append(section)\n",
    "            i += 1\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text from common PDF extraction artifacts.\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        text = re.sub(r'MIT OpenCourseWare.*?http://ocw\\.mit\\.edu', '', text)\n",
    "        text = re.sub(r'https?://[^\\s]+', '', text)  # Remove URLs\n",
    "        \n",
    "        text = re.sub(r'(?<=\\n)[\\s■◼▪◦•o*]+', '- ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def is_metadata(self, section: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Check if a section is metadata (not content).\"\"\"\n",
    "        combined = section[\"title\"] + \" \" + section[\"content\"]\n",
    "        metadata_patterns = [\n",
    "            r'MIT OpenCourseWare',\n",
    "            r'copyright',\n",
    "            r'https?://',\n",
    "            r'License',\n",
    "            r'Terms of Use',\n",
    "            r'Table of Contents',\n",
    "            r'Recapitulare',\n",
    "            r'Bibliografie',\n",
    "            r'References'\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, combined, re.IGNORECASE) for pattern in metadata_patterns)\n",
    "    \n",
    "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two text segments.\"\"\"\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def classify_section(self, section: Dict[str, Any]) -> str:\n",
    "        if not self.classifier:\n",
    "            # Fallback to pattern-based classification\n",
    "            return self.classify_by_patterns(section)\n",
    "        \n",
    "        # Use trained classifier\n",
    "        try:\n",
    "            # Use a shortened version to avoid tokenizer limits\n",
    "            text = section[\"content\"][:1000]  \n",
    "            result = self.classifier(text)[0]\n",
    "            return result[\"label\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Classification error: {e}\")\n",
    "            return self.classify_by_patterns(section)\n",
    "    \n",
    "    def classify_by_patterns(self, section: Dict[str, Any]) -> str:\n",
    "        \"\"\"Classify based on text patterns when model is not available.\"\"\"\n",
    "        title = section[\"title\"]\n",
    "        content = section[\"content\"]\n",
    "        combined = title + \"\\n\" + content\n",
    "        \n",
    "        # Check for code patterns\n",
    "        code_patterns = [\n",
    "            r'^\\s*(def|if|for|while|print|import|return)\\b',\n",
    "            r'==|!=|<=|>=|\\+=|-=|\\*=|/=',\n",
    "            r'\\brange\\(|\\bbreak\\b|\\breturn\\b',\n",
    "            r'^[a-zA-Z_][a-zA-Z0-9_]*\\s*=\\s*[^=]',\n",
    "            r'```python|```java|```c\\+\\+|```javascript',\n",
    "            r'>>> |In \\[\\d+\\]:'\n",
    "        ]\n",
    "        \n",
    "        if any(re.search(pattern, combined, re.MULTILINE) for pattern in code_patterns):\n",
    "            return 'code'\n",
    "            \n",
    "        # Check for mathematical content\n",
    "        math_patterns = [\n",
    "            r'(\\b[A-Z]\\([^\\)]+\\))|(\\b[a-z]\\([^\\)]+\\))', \n",
    "            r'[≤≥≠∈∉∑∏∫∂∇∞∝∀∃]',  \n",
    "            r'\\bproof\\b|\\btheorem\\b|\\blemma\\b|\\bclaim\\b',\n",
    "            r'\\blim\\b|\\bmax\\b|\\bmin\\b|\\bsup\\b|\\binf\\b',\n",
    "            r'[A-Za-z]_{\\d+}|[A-Za-z]_\\d',  \n",
    "            r'O\\([^)]*n[^)]*\\)',  \n",
    "        ]\n",
    "        \n",
    "        if any(re.search(pattern, combined, re.IGNORECASE | re.MULTILINE) for pattern in math_patterns):\n",
    "            return 'math'\n",
    "        \n",
    "        # Check for examples\n",
    "        example_patterns = [\n",
    "            r'EXAMPLE|for example',\n",
    "            r'e\\.g\\.',\n",
    "            r'^\\d+\\)\\s+'\n",
    "        ]\n",
    "        \n",
    "        if any(re.search(pattern, combined, re.IGNORECASE | re.MULTILINE) for pattern in example_patterns):\n",
    "            return 'example'\n",
    "        \n",
    "        # Default to context\n",
    "        return 'context'\n",
    "    \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect the language of a text.\"\"\"\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except LangDetectException:\n",
    "            return \"en\"  # Default to English\n",
    "    \n",
    "    def process_with_llm(self, section: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        # First classify the section\n",
    "        section_class = self.classify_section(section)\n",
    "        section[\"class\"] = section_class\n",
    "        \n",
    "        # Choose appropriate prompt based on classification\n",
    "        if section_class == \"code\":\n",
    "            prompt = self.code_prompt\n",
    "        elif section_class == \"math\":\n",
    "            prompt = self.math_prompt\n",
    "        else:\n",
    "            prompt = self.general_prompt\n",
    "        \n",
    "        # Create chain\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        \n",
    "        # Process with LLM\n",
    "        try:\n",
    "            result = chain.run(\n",
    "                title=section[\"title\"], \n",
    "                text=section[\"content\"],\n",
    "                source=section[\"source\"]\n",
    "            )\n",
    "            section[\"llm_output\"] = result\n",
    "            \n",
    "            # Extract structured data from LLM output\n",
    "            extracted = self.extract_fields_from_llm_output(result)\n",
    "            section.update(extracted)\n",
    "        except Exception as e:\n",
    "            print(f\"LLM processing error: {e}\")\n",
    "            section[\"llm_output\"] = f\"Error: {str(e)}\"\n",
    "            section[\"summary\"] = \"Failed to process with LLM\"\n",
    "        \n",
    "        return section\n",
    "    \n",
    "    def extract_fields_from_llm_output(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract structured fields from LLM output.\"\"\"\n",
    "        extracted = {}\n",
    "        \n",
    "        # Extract title\n",
    "        title_match = re.search(r'Title:\\s*(.*?)(?:\\n|$)', text)\n",
    "        if title_match:\n",
    "            extracted[\"processed_title\"] = title_match.group(1).strip()\n",
    "        \n",
    "        # Extract content/text\n",
    "        content_match = re.search(r'Content:\\s*(.*?)(?:Summary:|Key Concepts:|Main Topic:|Language:|Description:|$)', \n",
    "                                 text, re.DOTALL)\n",
    "        if content_match:\n",
    "            extracted[\"processed_content\"] = content_match.group(1).strip()\n",
    "        \n",
    "        # Extract summary\n",
    "        summary_match = re.search(r'Summary:\\s*(.*?)(?:Key Concepts:|Main Topic:|$)', text, re.DOTALL)\n",
    "        if summary_match:\n",
    "            extracted[\"summary\"] = summary_match.group(1).strip()\n",
    "        \n",
    "        # Extract key concepts\n",
    "        concepts_match = re.search(r'Key Concepts:\\s*(.*?)(?:Main Topic:|Language:|Description:|Source:|$)', text, re.DOTALL)\n",
    "        if concepts_match:\n",
    "            concepts = concepts_match.group(1).strip()\n",
    "\n",
    "            extracted[\"key_concepts\"] = [c.strip() for c in concepts.split(',')]\n",
    "        \n",
    "        # Extract main topic\n",
    "        topic_match = re.search(r'Main Topic:\\s*(.*?)(?:Source:|$)', text, re.DOTALL)\n",
    "        if topic_match:\n",
    "            extracted[\"main_topic\"] = topic_match.group(1).strip()\n",
    "        \n",
    "        lang_match = re.search(r'Language:\\s*(.*?)(?:Description:|$)', text)\n",
    "        if lang_match:\n",
    "            extracted[\"programming_language\"] = lang_match.group(1).strip()\n",
    "        \n",
    "        return extracted\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str, output_path: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a PDF file end-to-end.\n",
    "        \"\"\"\n",
    "        print(f\"Processing {pdf_path}...\")\n",
    "        \n",
    "        # Extract structured content\n",
    "        sections = self.extract_structured_content(pdf_path)\n",
    "        print(f\"Extracted {len(sections)} sections\")\n",
    "        \n",
    "        # Process each section with the LLM\n",
    "        processed_sections = []\n",
    "        for i, section in enumerate(sections):\n",
    "            print(f\"Processing section {i+1}/{len(sections)}: {section['title']}\")\n",
    "            processed = self.process_with_llm(section)\n",
    "            processed_sections.append(processed)\n",
    "        \n",
    "        # Save to JSON if output path provided\n",
    "        if output_path:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_sections, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Saved processed data to {output_path}\")\n",
    "        \n",
    "        return processed_sections\n",
    "    \n",
    "    def validate_llm_outputs(self, processed_sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Validate LLM outputs for hallucination by comparing against original text.\n",
    "        \"\"\"\n",
    "        for section in processed_sections:\n",
    "            # Skip sections with errors\n",
    "            if \"summary\" not in section or section[\"summary\"].startswith(\"Failed\"):\n",
    "                section[\"validation_score\"] = 0.0\n",
    "                section[\"hallucination_risk\"] = \"high\"\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity between original content and processed content\n",
    "            original_content = section[\"content\"]\n",
    "            processed_content = section.get(\"processed_content\", \"\")\n",
    "            summary = section.get(\"summary\", \"\")\n",
    "            \n",
    "            # Compare original content to processed content and summary\n",
    "            content_similarity = self.calculate_similarity(original_content, processed_content)\n",
    "            summary_similarity = self.calculate_similarity(original_content, summary)\n",
    "            \n",
    "            # Average the similarities\n",
    "            avg_similarity = (content_similarity + summary_similarity) / 2.0\n",
    "            \n",
    "            # Add validation score\n",
    "            section[\"validation_score\"] = round(avg_similarity, 2)\n",
    "            \n",
    "            # Label hallucination risk\n",
    "            if avg_similarity < 0.3:\n",
    "                section[\"hallucination_risk\"] = \"high\"\n",
    "            elif avg_similarity < 0.6:\n",
    "                section[\"hallucination_risk\"] = \"medium\"\n",
    "            else:\n",
    "                section[\"hallucination_risk\"] = \"low\"\n",
    "                \n",
    "        return processed_sections"
   ],
   "id": "d4904b74c540e87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def process_pdf_for_rag(pdf_path, output_path=None, llm_base_url=\"http://localhost:4500/v1\", \n",
    "                       llm_api_key=\"lm-studio\", llm_model=\"meta-llama-3.1-8b-instruct\"):\n",
    "    \"\"\"Process a PDF for RAG and return the processed data and metrics\"\"\"\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = ImprovedPDFProcessor(\n",
    "        llm_base_url=llm_base_url,\n",
    "        llm_api_key=llm_api_key,\n",
    "        llm_model=llm_model,\n",
    "        temperature=0.2 \n",
    "    )\n",
    "    \n",
    "    # Process PDF\n",
    "    processed_data = processor.process_pdf(pdf_path, output_path)\n",
    "    \n",
    "    validated_data = processor.validate_llm_outputs(processed_data)\n",
    "    \n",
    "    print(\"\\n===== PROCESSING SUMMARY =====\")\n",
    "    for i, section in enumerate(validated_data):\n",
    "        print(f\"\\nSection {i+1}: {section.get('title', 'Untitled')}\")\n",
    "        print(f\"Classification: {section.get('class', 'Unknown')}\")\n",
    "        print(f\"Main Topic: {section.get('main_topic', 'Unknown')}\")\n",
    "        print(f\"Validation Score: {section.get('validation_score', 0.0)} (Hallucination Risk: {section.get('hallucination_risk', 'Unknown')})\")\n",
    "    \n",
    "\n",
    "    sections_df = pd.DataFrame([\n",
    "        {\n",
    "            \"title\": section.get(\"title\", \"\"),\n",
    "            \"classification\": section.get(\"class\", \"unknown\"),\n",
    "            \"main_topic\": section.get(\"main_topic\", \"\"),\n",
    "            \"num_concepts\": len(section.get(\"key_concepts\", [])),\n",
    "            \"content_length\": len(section.get(\"content\", \"\")),\n",
    "            \"summary_length\": len(section.get(\"summary\", \"\")),\n",
    "            \"validation_score\": section.get(\"validation_score\", 0.0),\n",
    "            \"hallucination_risk\": section.get(\"hallucination_risk\", \"unknown\")\n",
    "        }\n",
    "        for section in validated_data\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n===== SECTIONS SUMMARY =====\")\n",
    "    print(sections_df)\n",
    "    \n",
    "    return validated_data, sections_df"
   ],
   "id": "2f5ee5c8a4c80ae6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pdf_path = \"data/slides/MIT6_0001F16_Lec8.pdf\" \n",
    "output_json = \"processed_data.json\" \n",
    "\n",
    "data, metrics = process_pdf_for_rag(pdf_path, output_json)\n",
    "\n",
    "\n",
    "print(\"\\nStep 2: Visualizing the results\")\n",
    "\n",
    "# Plot classification distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "metrics['classification'].value_counts().plot(kind='bar')\n",
    "plt.title('Content Classification Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot hallucination risk\n",
    "plt.figure(figsize=(10, 5))\n",
    "metrics['hallucination_risk'].value_counts().plot(kind='bar', color=['green', 'orange', 'red'])\n",
    "plt.title('Hallucination Risk Assessment')\n",
    "plt.xlabel('Risk Level')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7682552ac47d3530"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_directory(pdf_dir, output_dir='processed_data'):\n",
    "    \"\"\"Process all PDFs in a directory\"\"\"\n",
    "    import os\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_data = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        output_json = os.path.join(output_dir, f\"{os.path.splitext(pdf_file)[0]}_processed.json\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nProcessing {pdf_file}...\")\n",
    "            data, metrics = process_pdf_for_rag(pdf_path, output_json)\n",
    "            all_data.extend(data)\n",
    "            all_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "    \n",
    "    # Combine all metrics\n",
    "    if all_metrics:\n",
    "        combined_metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "        combined_metrics.to_csv(os.path.join(output_dir, \"combined_metrics.csv\"), index=False)\n",
    "        print(f\"\\nProcessed {len(pdf_files)} PDF files. Combined metrics saved to {os.path.join(output_dir, 'combined_metrics.csv')}\")\n",
    "        return all_data, combined_metrics\n",
    "    else:\n",
    "        print(\"No files were processed successfully.\")\n",
    "        return [], pd.DataFrame()\n"
   ],
   "id": "83f54e0163f18e0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def prepare_for_rag(processed_data):\n",
    "    \"\"\"\n",
    "    Prepare processed data for use in a RAG system by:\n",
    "    1. Filtering out high hallucination risk content\n",
    "    2. Structuring data for embedding and retrieval\n",
    "    3. Creating metadata for improved retrieval\n",
    "    \"\"\"\n",
    "    rag_documents = []\n",
    "    \n",
    "    for item in processed_data:\n",
    "        # Skip items with high hallucination risk\n",
    "        if item.get('hallucination_risk') == 'high':\n",
    "            continue\n",
    "            \n",
    "        content = item.get('summary', item.get('processed_content', item.get('content', '')))\n",
    "        \n",
    "        # Create metadata for better retrieval\n",
    "        metadata = {\n",
    "            'source': item.get('source', ''),\n",
    "            'title': item.get('processed_title', item.get('title', '')),\n",
    "            'classification': item.get('class', 'unknown'),\n",
    "            'main_topic': item.get('main_topic', ''),\n",
    "            'key_concepts': item.get('key_concepts', []),\n",
    "        }\n",
    "        \n",
    "        # Create a retrieval-friendly document\n",
    "        rag_document = {\n",
    "            'content': content,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "        rag_documents.append(rag_document)\n",
    "    \n",
    "    print(f\"Prepared {len(rag_documents)} documents for RAG (filtered out {len(processed_data) - len(rag_documents)} high-risk items)\")\n",
    "    return rag_documents\n"
   ],
   "id": "9d87582a9e2adea6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def export_for_embeddings(rag_documents, output_file=\"rag_documents.jsonl\"):\n",
    "    \"\"\"Export in JSONL format suitable for embedding generation\"\"\"\n",
    "    import json\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for doc in rag_documents:\n",
    "            f.write(json.dumps(doc) + '\\n')\n",
    "    \n",
    "    print(f\"Exported {len(rag_documents)} documents to {output_file}\")\n"
   ],
   "id": "54787b9a3e3ac49a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pdf_processor = ImprovedPDFProcessor()\n",
    "metadata_extractor = CSMetadataExtractor()\n",
    "embedding_model = \"sentence-transformers/LaBSE\"\n",
    "vector_db_path = \"./rag/chroma_db\"\n",
    "\n",
    "# Set up ChromaDB with LaBSE embeddings\n",
    "chroma_client = chromadb.PersistentClient(path=vector_db_path)\n",
    "sentence_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=embedding_model\n",
    ")\n",
    "\n",
    "# Create or get collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"cs_multilingual_docs\", \n",
    "    embedding_function=sentence_ef\n",
    ")\n",
    "def process_and_index_pdf(pdf_path):\n",
    "    # Process the PDF\n",
    "    processed_data = pdf_processor.process_pdf(pdf_path)\n",
    "    \n",
    "    # Add to vector database with enhanced metadata\n",
    "    for i, section in enumerate(processed_data):\n",
    "        # Skip high hallucination risk content\n",
    "        if section.get('hallucination_risk') == 'high':\n",
    "            continue\n",
    "            \n",
    "        # Extract CS-specific metadata\n",
    "        cs_metadata = metadata_extractor.extract_metadata(\n",
    "            text=section.get('content', ''),\n",
    "            title=section.get('title', ''),\n",
    "            lang=section.get('language', 'en')\n",
    "        )\n",
    "        \n",
    "        # Combine metadata\n",
    "        combined_metadata = {\n",
    "            **{k: v for k, v in section.items() if k not in ['content', 'processed_content', 'summary']},\n",
    "            **cs_metadata\n",
    "        }\n",
    "        \n",
    "        # Choose the best text representation for embedding\n",
    "        if section.get('summary') and len(section.get('summary')) > 50:\n",
    "            content_for_embedding = section['summary']\n",
    "        else:\n",
    "            content_for_embedding = section.get('processed_content', section.get('content', ''))\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            documents=[content_for_embedding],\n",
    "            metadatas=[combined_metadata],\n",
    "            ids=[f\"{os.path.basename(pdf_path)}_section_{i}\"]\n",
    "        )"
   ],
   "id": "2e80f38ca6b3181e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
