{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T05:24:49.976096Z",
     "start_time": "2025-04-28T05:24:49.971955Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install transformers datasets peft accelerate",
   "id": "bb62336fb1cfd7a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T05:24:49.995314Z",
     "start_time": "2025-04-28T05:24:49.987614Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124",
   "id": "d260558cd7de91ec",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T05:24:50.119862Z",
     "start_time": "2025-04-28T05:24:50.114778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ],
   "id": "e6a7c2367b5bfb64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T05:27:50.950884Z",
     "start_time": "2025-04-28T05:27:46.859295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    TrOCRProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to data\n",
    "DATA_DIR = \"data/handwritten\"\n",
    "RO_CSV_DIR= os.path.join(DATA_DIR, \"handwriting_dataset_ro_splits\")\n",
    "EN_CSV_DIR = os.path.join(DATA_DIR, \"iam\")\n",
    "RO_TRAIN_CSV = os.path.join(RO_CSV_DIR, \"ro_train.csv\")\n",
    "RO_VAL_CSV = os.path.join(RO_CSV_DIR, \"ro_val.csv\")\n",
    "RO_TEST_CSV = os.path.join(RO_CSV_DIR, \"ro_test.csv\")\n",
    "EN_TRAIN_CSV = os.path.join(EN_CSV_DIR, \"train.csv\")\n",
    "EN_VAL_CSV = os.path.join(EN_CSV_DIR, \"validation.csv\")\n",
    "EN_TEST_CSV = os.path.join(EN_CSV_DIR, \"test.csv\")\n",
    "MAX_SAMPLES_PER_LANGUAGE = 1000  # Set to None for full dataset\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"microsoft/trocr-base-handwritten\"\n",
    "OUTPUT_DIR = \"trocr_lora_bilingual_improved\"\n",
    "\n",
    "def normalize_romanian_text(text):\n",
    "    \"\"\"Normalize Romanian text to standardize diacritics\"\"\"\n",
    "    replacements = {\n",
    "        'ş': 'ș', 'ţ': 'ț',  \n",
    "        'ã': 'ă', 'å': 'ă', \n",
    "        'â': 'ă',\n",
    "        'î': 'ă',\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Image preprocessing for Romanian handwriting\n",
    "def preprocess_image_for_romanian(image):\n",
    "    \"\"\"Apply preprocessing optimized for Romanian handwriting\"\"\"\n",
    "    gray = image.convert('L')\n",
    "    \n",
    "    from PIL import ImageEnhance\n",
    "    enhancer = ImageEnhance.Contrast(gray)\n",
    "    enhanced = enhancer.enhance(1.5) \n",
    "    \n",
    "    return enhanced.convert('RGB')\n",
    "\n",
    "logger.info(\"Loading processor and model...\")\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "original_encoder_forward = model.encoder.forward\n",
    "\n",
    "def safe_encoder_forward(self, pixel_values=None, **kwargs):\n",
    "    return original_encoder_forward(pixel_values=pixel_values)\n",
    "\n",
    "model.encoder.forward = lambda pixel_values=None, **kwargs: safe_encoder_forward(model.encoder, pixel_values=pixel_values)\n",
    "\n",
    "# Configure LoRA with more capacity for cross-lingual learning\n",
    "logger.info(\"Configuring LoRA...\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=16,  \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "logger.info(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n",
    "\n",
    "class OCRDatasetWithLang(Dataset):\n",
    "    def __init__(self, csv_file, processor, language_id, max_target_length=128, image_dir=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.image_dir = image_dir\n",
    "        self.language_id = language_id\n",
    "        \n",
    "        assert \"image\" in self.df.columns, f\"'image' column missing in {csv_file}\"\n",
    "        assert \"text\" in self.df.columns, f\"'text' column missing in {csv_file}\"\n",
    "        \n",
    "        self.df = self.df.dropna(subset=[\"image\", \"text\"])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image\"]\n",
    "        text = str(self.df.iloc[idx][\"text\"])\n",
    "        \n",
    "        if self.language_id == 'ro':\n",
    "            text = normalize_romanian_text(text)\n",
    "        \n",
    "        if self.image_dir and not os.path.isabs(img_path):\n",
    "            img_path = os.path.join(self.image_dir, img_path)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if self.language_id == 'ro':\n",
    "                image = preprocess_image_for_romanian(image)\n",
    "                \n",
    "            pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "            \n",
    "            labels = self.processor.tokenizer(\n",
    "                text, \n",
    "                padding=\"max_length\", \n",
    "                max_length=self.max_target_length,\n",
    "                truncation=True\n",
    "            ).input_ids\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"labels\": torch.tensor(labels),\n",
    "                \"language_id\": self.language_id\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "class CombinedOCRDataset(Dataset):\n",
    "    def __init__(self, ro_dataset, en_dataset):\n",
    "        self.ro_dataset = ro_dataset\n",
    "        self.en_dataset = en_dataset\n",
    "        self.total_len = len(ro_dataset) + len(en_dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.ro_dataset):\n",
    "            return self.ro_dataset[idx]\n",
    "        else:\n",
    "            return self.en_dataset[idx - len(self.ro_dataset)]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    has_lang_id = all('language_id' in item for item in batch)\n",
    "    \n",
    "    result = {\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
    "    }\n",
    "    \n",
    "    if has_lang_id:\n",
    "        result['language_id'] = [item['language_id'] for item in batch]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create datasets\n",
    "logger.info(\"Creating datasets...\")\n",
    "IMAGE_DIR = None  \n",
    "\n",
    "train_ro_dataset = OCRDatasetWithLang(RO_TRAIN_CSV, processor, 'ro', image_dir=IMAGE_DIR)\n",
    "val_ro_dataset = OCRDatasetWithLang(RO_VAL_CSV, processor, 'ro', image_dir=IMAGE_DIR)\n",
    "test_ro_dataset = OCRDatasetWithLang(RO_TEST_CSV, processor, 'ro', image_dir=IMAGE_DIR)\n",
    "\n",
    "train_en_dataset = OCRDatasetWithLang(EN_TRAIN_CSV, processor, 'en', image_dir=IMAGE_DIR)\n",
    "val_en_dataset = OCRDatasetWithLang(EN_VAL_CSV, processor, 'en', image_dir=IMAGE_DIR)\n",
    "test_en_dataset = OCRDatasetWithLang(EN_TEST_CSV, processor, 'en', image_dir=IMAGE_DIR)\n",
    "\n",
    "if MAX_SAMPLES_PER_LANGUAGE is not None:\n",
    "    train_ro_dataset.df = train_ro_dataset.df.head(MAX_SAMPLES_PER_LANGUAGE)\n",
    "    train_en_dataset.df = train_en_dataset.df.head(MAX_SAMPLES_PER_LANGUAGE)\n",
    "    # Also limit validation sets\n",
    "    val_limit = max(50, int(MAX_SAMPLES_PER_LANGUAGE * 0.1))\n",
    "    val_ro_dataset.df = val_ro_dataset.df.head(val_limit)\n",
    "    val_en_dataset.df = val_en_dataset.df.head(val_limit)\n",
    "\n",
    "logger.info(f\"Romanian training samples: {len(train_ro_dataset)}\")\n",
    "logger.info(f\"English training samples: {len(train_en_dataset)}\")\n",
    "\n",
    "# Combine datasets using our custom combiner\n",
    "combined_train_dataset = CombinedOCRDataset(train_ro_dataset, train_en_dataset)\n",
    "combined_val_dataset = CombinedOCRDataset(val_ro_dataset, val_en_dataset)\n",
    "\n",
    "logger.info(f\"Combined training samples: {len(combined_train_dataset)}\")\n",
    "logger.info(f\"Combined validation samples: {len(combined_val_dataset)}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=50,  # more frequent logging\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2, \n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),  \n",
    "    fp16_opt_level=\"O1\",  \n",
    "    fp16_full_eval=False,  \n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    try:\n",
    "        from evaluate import load\n",
    "        cer_metric = load(\"cer\")\n",
    "        wer_metric = load(\"wer\")\n",
    "    except (ImportError, ModuleNotFoundError):\n",
    "        logger.warning(\"Could not load metrics.\")\n",
    "        return {\"cer\": 0.0, \"wer\": 0.0}\n",
    "    \n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    \n",
    "    pred_ids_fixed = np.where(pred_ids < 0, processor.tokenizer.pad_token_id, pred_ids)\n",
    "    \n",
    "    try:\n",
    "        pred_str = processor.batch_decode(pred_ids_fixed, skip_special_tokens=True)\n",
    "        \n",
    "        labels_ids[labels_ids < 0] = processor.tokenizer.pad_token_id\n",
    "        label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        \n",
    "        cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        \n",
    "        return {\"cer\": cer, \"wer\": wer}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing metrics: {e}\")\n",
    "        return {\"cer\": 1.0, \"wer\": 1.0} \n",
    "\n",
    "class BilingualTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        model_inputs = inputs.copy()\n",
    "        \n",
    "        language_ids = None\n",
    "        if 'language_id' in model_inputs:\n",
    "            language_ids = model_inputs.pop('language_id')\n",
    "        \n",
    "        outputs = model(**model_inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Apply weight to Romanian samples if language IDs are available\n",
    "        if language_ids is not None:\n",
    "            # If there are any Romanian samples, increase the loss weight\n",
    "            if 'ro' in language_ids:\n",
    "                ro_sample_weight = 2.5\n",
    "                ro_ratio = language_ids.count('ro') / len(language_ids)\n",
    "                weighted_factor = 1.0 + (ro_ratio * (ro_sample_weight - 1.0))\n",
    "                loss = loss * weighted_factor\n",
    "                logger.info(f\"Batch with {ro_ratio*100:.1f}% Romanian samples, loss weight: {weighted_factor:.2f}\")\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = BilingualTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=combined_val_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "def evaluate_model(model, test_dataset, language):\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=4,  \n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        from evaluate import load\n",
    "        cer_metric = load(\"cer\")\n",
    "        wer_metric = load(\"wer\")\n",
    "    except ImportError:\n",
    "        logger.error(\"Could not load metrics. Please install jiwer: pip install jiwer\")\n",
    "        return 1.0, 1.0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=f\"Evaluating {language}\"):\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            eval_batch = batch.copy()\n",
    "            if 'language_id' in eval_batch:\n",
    "                del eval_batch['language_id']\n",
    "                \n",
    "            pixel_values = eval_batch[\"pixel_values\"].to(device)\n",
    "            labels = eval_batch[\"labels\"]\n",
    "            \n",
    "            try:\n",
    "                generated_ids = model.generate(pixel_values=pixel_values, max_length=64)\n",
    "                \n",
    "                pred_str = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "                \n",
    "                all_preds.extend(pred_str)\n",
    "                all_labels.extend(label_str)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during evaluation: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if all_preds:\n",
    "        cer = cer_metric.compute(predictions=all_preds, references=all_labels)\n",
    "        wer = wer_metric.compute(predictions=all_preds, references=all_labels)\n",
    "        \n",
    "        logger.info(f\"{language} Test CER: {cer:.4f}\")\n",
    "        logger.info(f\"{language} Test WER: {wer:.4f}\")\n",
    "        \n",
    "        return cer, wer\n",
    "    else:\n",
    "        logger.error(f\"No predictions were generated for {language}\")\n",
    "        return 1.0, 1.0\n",
    "\n",
    "# Evaluate and log the results\n",
    "# if os.path.exists(OUTPUT_DIR):\n",
    "#     logger.info(\"Evaluating on test sets...\")\n",
    "#     ro_cer, ro_wer = evaluate_model(model, test_ro_dataset, \"Romanian\")\n",
    "#     en_cer, en_wer = evaluate_model(model, test_en_dataset, \"English\")\n",
    "# \n",
    "#     logger.info(\"Training and evaluation complete!\")\n",
    "#     logger.info(f\"Final Romanian CER: {ro_cer:.4f}, WER: {ro_wer:.4f}\")\n",
    "#     logger.info(f\"Final English CER: {en_cer:.4f}, WER: {en_wer:.4f}\")"
   ],
   "id": "6cfd592747b76fb9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Loading processor and model...\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Configuring LoRA...\n",
      "INFO:__main__:Trainable parameters: None\n",
      "INFO:__main__:Creating datasets...\n",
      "INFO:__main__:Romanian training samples: 1000\n",
      "INFO:__main__:English training samples: 1000\n",
      "INFO:__main__:Combined training samples: 2000\n",
      "INFO:__main__:Combined validation samples: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,013,504 || all params: 338,935,296 || trainable%: 1.4792\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T05:35:18.664432Z",
     "start_time": "2025-04-28T05:35:18.654909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_use_model(model_path):\n",
    "    \"\"\"Load a saved LoRA model and run inference\"\"\"\n",
    "    # Ensure we're using a local path\n",
    "    model_path = os.path.abspath(model_path)\n",
    "    \n",
    "    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "    base_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "    \n",
    "    # Set required token IDs\n",
    "    base_model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    base_model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    original_encoder_forward = base_model.encoder.forward\n",
    "    def safe_encoder_forward(self, pixel_values=None, **kwargs):\n",
    "        return original_encoder_forward(pixel_values=pixel_values)\n",
    "    base_model.encoder.forward = lambda pixel_values=None, **kwargs: safe_encoder_forward(base_model.encoder, pixel_values=pixel_values)\n",
    "    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model, \n",
    "        model_path,\n",
    "        local_files_only=True  \n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def ocr_inference(model, processor, image_path, is_romanian=False):\n",
    "    \"\"\"Perform OCR inference on a single image\"\"\"\n",
    "    # Load and optionally preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Apply Romanian-specific preprocessing if needed\n",
    "    if is_romanian:\n",
    "        image = preprocess_image_for_romanian(image)\n",
    "    \n",
    "    # Get model predictions\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values=pixel_values, max_length=64)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Apply Romanian normalization if needed\n",
    "    if is_romanian:\n",
    "        generated_text = normalize_romanian_text(generated_text)\n",
    "    \n",
    "    return generated_text\n",
    "\n"
   ],
   "id": "610f71929b725375",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T05:38:29.828052Z",
     "start_time": "2025-04-28T05:38:25.894169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, processor = load_and_use_model(\"models/trocr_lora_bilingual_improved\")\n",
    "text = ocr_inference(model, processor, \"img.png\", is_romanian=True)\n",
    "print(f\"OCR Result: {text}\")"
   ],
   "id": "7d52fce0c2e98753",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Result: Stone- Grea\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inca nu functioneaza bine trebuie sa adaug la db scris real.",
   "id": "ef3a2fd9ddfa4435"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
